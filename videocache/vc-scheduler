#!/usr/bin/env python
#
# (C) Copyright White Magnet Software Private Limited
# Company Website : http://whitemagnet.com/
# Product Website : http://cachevideos.com/
#

__author__ = """Kulbir Saini <saini@saini.co.in>"""
__docformat__ = 'plaintext'

from common import *
from vcredis import VideoFile, VideoQueue, Youtube, AccessLogQueue
from fsop import *
from store import *
from vcdaemon import VideocacheDaemon
from vcoptions import VideocacheOptions
from vcsysinfo import get_all_info

from optparse import OptionParser

import cgi
import cookielib
import glob
import pwd
import random
import shutil
import signal
import socket
import sys
import threading
import time
import traceback
import urllib2
import urlparse

# Cookie processor and default socket timeout
cj = cookielib.CookieJar()
urllib2.install_opener(urllib2.build_opener(urllib2.HTTPCookieProcessor(cj)))
socket.setdefaulttimeout(90)

# Alias urllib2.open to urllib2.urlopen
urllib2.open = urllib2.urlopen

def info(params = {}):
    if o.enable_scheduler_log and o.vcs_logger:
        params.update({ 'logformat' : o.scheduler_logformat, 'timeformat' : o.timeformat, 'levelname' : LOG_LEVEL_INFO, 'process_id' : process_id })
        o.vcs_logger.info(build_message(params))

def error(params = {}):
    if o.enable_scheduler_log and o.vcs_logger:
        params.update({ 'logformat' : o.scheduler_logformat, 'timeformat' : o.timeformat, 'levelname' : LOG_LEVEL_ERR, 'process_id' : process_id })
        o.vcs_logger.error(build_message(params))

def warn(params = {}):
    if o.enable_scheduler_log and o.vcs_logger:
        params.update({ 'logformat' : o.scheduler_logformat, 'timeformat' : o.timeformat, 'levelname' : LOG_LEVEL_WARN, 'process_id' : process_id })
        o.vcs_logger.debug(build_message(params))

def cleaner_info(params = {}):
    if o.enable_cleaner_log and o.vcc_logger:
        params.update({ 'logformat' : o.cleaner_logformat, 'timeformat' : o.timeformat, 'levelname' : LOG_LEVEL_INFO, 'process_id' : process_id})
        o.vcc_logger.info(build_message(params))

def cleaner_error(params = {}):
    if o.enable_cleaner_log and o.vcc_logger:
        params.update({ 'logformat' : o.cleaner_logformat, 'timeformat' : o.timeformat, 'levelname' : LOG_LEVEL_ERR, 'process_id' : process_id})
        o.vcc_logger.error(build_message(params))

def cleaner_warn(params = {}):
    if o.enable_cleaner_log and o.vcc_logger:
        params.update({ 'logformat' : o.cleaner_logformat, 'timeformat' : o.timeformat, 'levelname' : LOG_LEVEL_WARN, 'process_id' : process_id})
        o.vcc_logger.debug(build_message(params))

def trace(params = {}):
    if o.enable_trace_log and o.trace_logger:
        params.update({ 'logformat' : o.trace_logformat, 'timeformat' : o.timeformat, 'process_id' : process_id })
        o.trace_logger.info(build_message(params))

def ent(params = {}):
    error(params)
    params.update({ 'message' : traceback.format_exc() })
    trace(params)

def cent(params = {}):
    cleaner_error(params)
    params.update({ 'message' : traceback.format_exc() })
    trace(params)

def wnt(params = {}):
    error(params)
    params.update({ 'message' : traceback.format_exc() })
    trace(params)

class TaskThread(threading.Thread):
    def __init__(self, *args, **kwargs):
        threading.Thread.__init__(self, *args, **kwargs)
        self._real_run = self.run
        self.run = self._wrap_run

    def _wrap_run(self):
        try:
            self._real_run()
        except Exception, e:
            ent({'code' : 'TASK_THREAD_ERR', 'message' : 'Error in task thread.', 'debug' : str(e)})

class LoggingThread(threading.Thread):
    def __init__(self, *args, **kwargs):
        threading.Thread.__init__(self, *args, **kwargs)
        self._real_run = self.run
        self.run = self._wrap_run

    def _wrap_run(self):
        try:
            self._real_run()
        except Exception, e:
            ent({'code' : 'CACHE_THREAD_ERR', 'message' : 'Error in cache thread.', 'debug' : str(e)})

def get_random_user_agent():
    return o.std_headers[random.randint(0, o.num_std_headers - 1)]

def migrate_to_new_youtube_video_ids(website_id, old_video_id, new_video_id):
    if old_video_id and new_video_id:
        for youtube_dir in o.base_dirs[website_id]:
            for filename in glob.glob(os.path.join(youtube_dir, old_video_id + '*')):
                try:
                    move_file(filename, filename.replace(old_video_id, new_video_id))
                except:
                    pass

def cache_remote_url(remote_url, target_file, params = {}):
    try:
        video_id = params.get('video_id', '-')
        website_id = params.get('website_id', '-')
        http_headers = params.get('http_headers', get_random_user_agent())
        proxy = params.get('proxy', o.proxy_server)
        max_cache_speed = params.get('max_cache_speed', o.max_cache_speed)
        min_video_size = params.get('min_video_size', o.min_video_size)
        max_video_size = params.get('max_video_size', o.max_video_size)
        socket_read_block_size = params.get('socket_read_block_size', o.socket_read_block_size)
        log_error = params.get('log_error', True)

        if not http_headers.has_key('User-Agent'):
            http_headers.update(get_random_user_agent())

        opener = urllib2
        if proxy:
            opener = urllib2.build_opener(urllib2.ProxyHandler({ 'http' : proxy, 'https' : proxy, 'ftp' : proxy }))

        request = urllib2.Request(remote_url, None, http_headers)

        conn = opener.open(request)
        status = False
        try:
            conn_info = conn.info()
            video_size = int(conn_info.get('content-length', -1))
            if video_size == -1:
                if o.force_video_size == 1:
                    info({ 'code' : 'SIZE_NOT_FOUND', 'message' : 'Video size was not given by web server. Skipping.', 'video_id' : video_id, 'website_id' : website_id })
                    status = True
            else:
                if max_video_size != 0 and video_size > max_video_size:
                    info({ 'code' : 'VIDEO_TOO_LARGE', 'message' : 'Video size is large than the max size allowed. Skipping.', 'video_id' : video_id, 'website_id' : website_id, 'size' : video_size })
                    status = True
                if min_video_size != 0 and video_size < min_video_size:
                    info({ 'code' : 'VIDEO_TOO_SMALL', 'message' : 'Video size is smaller than the min size allowed. Skipping.', 'video_id' : video_id, 'website_id' : website_id, 'size' : video_size })
                    status = True
        except Exception, e:
            if log_error: wnt({ 'code' : 'CONNECTION_INFO_ERR', 'message' : 'Error while getting connection info.', 'debug' : str(e), 'video_id' : video_id, 'website_id' : website_id })

        if status:
            conn.close()
            return False

        filedesc = None
        start_time = time.time()
        downloaded = 0
        while True:
            block = conn.read(socket_read_block_size)
            if len(block) == 0:
                break
            if not filedesc:
                filedesc = open(target_file, 'wb')
            filedesc.write(block)

            downloaded += len(block)
            while max_cache_speed > 0 and downloaded / (time.time() - start_time) > max_cache_speed:
                time.sleep(0.1)
        if filedesc:
            filedesc.close()
        conn.close()

        target_file_size = os.path.getsize(target_file)
        if video_size != -1 and target_file_size < 0.97 * video_size:
            if log_error: error({ 'code' : 'PARTIAL_CACHE_ERR', 'message' : 'Video could not be cached completely. Expected: ' + str(video_size) + ', Got: ' + str(target_file_size), 'video_id' : video_id, 'website_id' : website_id })
            remove_file(target_file)
            return False
    except urllib2.HTTPError, e:
        remove_file(target_file)
        try:
            if log_error: ent({ 'code' : 'CACHE_HTTP_ERR', 'message' : 'HTTP error : ' + str(e.code) + '. An error occured while caching the video at '  + remote_url + '.', 'debug' : str(e), 'video_id' : video_id, 'website_id' : website_id })
            return False
        except:
            if log_error: ent({ 'code' : 'CACHE_HTTP_ERR', 'message' : 'HTTP error. An error occured while caching the video at '  + remote_url + '.', 'debug' : str(e), 'video_id' : video_id, 'website_id' : website_id })
            return False
    except Exception, e:
        remove_file(target_file)
        if log_error: ent({ 'code' : 'CACHE_ERR', 'message' : 'Could not cache the video at ' + remote_url + '.', 'debug' : str(e), 'video_id' : video_id, 'website_id' : website_id })
        return False
    return True

def cache_youtube_video(website_id, video_id, fmt = '', url = '', cur_cache_dir = None):
    try:
        if not fmt: fmt = '34'
        if not cur_cache_dir:
            warn({ 'code' : 'VIDEO_INFO_WARN', 'message' : 'Enough video information was not available in cache thread.', 'website_id' : website_id, 'video_id' : video_id })
            return

        proxy = o.proxy_server
        http_headers = get_random_user_agent()

        opener = urllib2
        if proxy:
            opener = urllib2.build_opener(urllib2.ProxyHandler({ 'http' : proxy, 'https' : proxy, 'ftp' : proxy }))

        video_info = None
        for el in ['&el=detailpage', '&el=embedded', '&el=vevo', '']:
            info_url = 'http://www.youtube.com/get_video_info?video_id=%s%s&ps=default&eurl=&gl=US&hl=en' % (video_id, el)
            request = urllib2.Request(info_url, None, http_headers)
            try:
                video_info = cgi.parse_qs(opener.open(request).read())
                try:
                    if int(video_info.get('view_count', [0])[0]) < o.min_youtube_views:
                        info({ 'code' : 'VIEW_COUNT_LOW', 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Video has not received enough views and will not be cached.' })
                        return
                except Exception, e:
                    pass

                if 'url_encoded_fmt_stream_map' in video_info:
                    break
            except Exception, e:
                ent({'code' : 'VIDEO_INFO_FETCH_ERR', 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Error occured while fetching video info from ' + info_url, 'debug' : str(e) })
        else:
            warn({ 'code' : 'VIDEO_INFO_FETCH_WARN', 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Could not fetch required info to cache video. Please report this if it happens very frequently.' })
            return

        try:
            url_map = {}
            [url_map.update({i['itag'][0] : i['url'][0] + '&signature=' + i['sig'][0]}) for i in [cgi.parse_qs(i) for i in video_info['url_encoded_fmt_stream_map'][0].split(',')]]
            alternate_vid = get_youtube_video_id(url_map.values()[0])
            if alternate_vid and len(alternate_vid) == 16:
                migrate_to_new_youtube_video_ids(website_id, alternate_vid, video_id)
        except Exception, e:
            ent({ 'code' : 'URL_EXTRACTION_ERROR', 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Error while extracting video urls.', 'debug' : str(e) })
            return

        cache_dir = os.path.join(cur_cache_dir, o.website_cache_dir[website_id])
        tmp_dir = os.path.join(cur_cache_dir, o.temp_dir)

        if url_map.has_key(fmt):
            stop = False
            if o.youtube_formats.has_key(fmt) and o.youtube_formats[fmt]['res'] > o.max_youtube_video_quality:
                stop = True
            if not stop and o.enable_youtube_html5_videos == 0 and o.youtube_formats[fmt]['cat'] in ['webm', 'webm_3d']:
                stop = True
            if not stop and o.enable_youtube_3d_videos == 0 and o.youtube_formats[fmt]['cat'] in ['regular_3d', 'webm_3d']:
                stop = True
            if stop:
                return

            if youtube_cached_url(o, video_id, website_id, fmt)[0]:
                info({ 'code' : 'VIDEO_EXISTS', 'website_id' : website_id, 'video_id' : video_id, 'message' : 'FORMAT ' + fmt + ' Video already exists.' })
            else:
                video_url = url_map[fmt]
                filename = get_youtube_filename(o, video_id, fmt)
                video_path = os.path.join(cache_dir, filename)
                tmp_path = os.path.join(tmp_dir, filename)

                if cache_remote_url(video_url, tmp_path, {'http_headers' : http_headers, 'video_id' : video_id, 'website_id' : website_id}):
                    size = os.path.getsize(tmp_path)
                    move_file(tmp_path, video_path)
                    os.chmod(video_path, o.file_mode)
                    os.utime(video_path, None)
                    info({ 'code' : 'VIDEO_CACHED', 'video_id' : video_id, 'website_id' : website_id, 'size' : size, 'message' : 'FORMAT ' + fmt + ' Video was cached successfully at ' + video_path })
                    video_file.add_info(website_id, filename, cur_cache_dir)
        else:
            info({ 'code' : 'UNSUPPORTED_FORMAT', 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Background caching of format ' + str(fmt) + ' is not supported' })
    except Exception, e:
        ent({'code' : 'CACHE_THREAD_ERR', 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Error in cache thread.'})

def cache_generalized(website_id, video_id, fmt = '', url = '', cur_cache_dir = None):
    try:
        if not (url and cur_cache_dir):
            warn({ 'code' : 'VIDEO_INFO_WARN', 'message' : 'Enough video information was not available in cache thread.', 'website_id' : website_id, 'video_id' : video_id })
            return

        if fmt is None: fmt = ''
        cache_dir = os.path.join(cur_cache_dir, o.website_cache_dir[website_id])
        tmp_dir = os.path.join(cur_cache_dir, o.temp_dir)
        video_path = os.path.join(cache_dir, video_id)
        tmp_path = os.path.join(tmp_dir, video_id)

        original_url = url
        url = refine_url(url, o.arg_drop_list[website_id])
        try:
            if eval(website_id + '_cached_url(o, video_id, website_id, fmt)')[0]:
                info({ 'code' : 'VIDEO_EXISTS', 'message' : 'Video already exists', 'website_id' : website_id, 'video_id' : video_id })
            else:
                if cache_remote_url(url, tmp_path, {'video_id' : video_id, 'website_id' : website_id}):
                    size = os.path.getsize(tmp_path)
                    move_file(tmp_path, video_path)
                    os.chmod(video_path, o.file_mode)
                    os.utime(video_path, None)
                    info({ 'code' : 'VIDEO_CACHED', 'video_id' : video_id, 'website_id' : website_id, 'size' : size, 'message' : 'Video was cached successfully at ' + video_path })
                    video_file.add_info(website_id, video_id, cur_cache_dir)
        except Exception, e:
            ent({ 'code' : 'URL_CACHE_ERR', 'message' : 'Failed to cache video at ' + original_url + '.', 'video_id' : video_id, 'website_id' : website_id })
    except Exception, e:
        ent({'code' : 'CACHE_THREAD_ERR', 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Error in cache thread.'})

def cache_android_video(video): pass

def cache_dailymotion_video(video): pass

cache_aol_video = cache_generalized
cache_bing_video = cache_generalized
cache_bliptv_video = cache_generalized
cache_breakcom_video = cache_generalized
cache_facebook_video = cache_generalized
cache_imdb_video = cache_generalized
cache_metacafe_video = cache_generalized
cache_myspace_video = cache_generalized
cache_rutube_video = cache_generalized
cache_veoh_video = cache_generalized
cache_videobash_video = cache_generalized
cache_vimeo_video = cache_generalized
cache_vkcom_video = cache_generalized
cache_vube_video = cache_generalized
cache_wrzuta_video = cache_generalized
cache_youku_video = cache_generalized

# Pr0n sites
cache_extremetube_video = cache_generalized
cache_hardsextube_video = cache_generalized
cache_keezmovies_video = cache_generalized
cache_pornhub_video = cache_generalized
cache_redtube_video = cache_generalized
cache_slutload_video = cache_generalized
cache_spankwire_video = cache_generalized
cache_tube8_video = cache_generalized
cache_xhamster_video = cache_generalized
cache_xtube_video = cache_generalized
cache_xvideos_video = cache_generalized
cache_youporn_video = cache_generalized

def refine_cache_periods():
    global cache_periods
    cache_periods = []
    if o.cache_periods == False:
        warn({ 'code' : 'CACHE_PERIOD_WARN', 'message' : 'Error in parsing the value of the option cache_period. Ignoring. Please set cache_period option in /etc/videocache.conf properly and restart vc-scheduler.'})
        cache_periods = []
    elif o.cache_periods == None:
        cache_periods = []
    else:
        for cp in o.cache_periods:
            if cp['start'][0] > cp['end'][0]:
                warn({ 'code' : 'CACHE_PERIOD_WARN', 'message' : 'A time period mentioned using cache_period option is not valid. Ignoring. Please set cache_period option in /etc/videocache.conf properly and restart vc-scheduler.', 'debug' : cache_period_h2s(cp)})
            else:
                cache_periods.append(cp)

def is_caching_time():
    if len(cache_periods) == 0:
        return True

    t = time.localtime()
    for cp in cache_periods:
        start_time = int('%02d%02d' % (cp['start'][0], cp['start'][1]))
        end_time = int('%02d%02d' % (cp['end'][0], cp['end'][1]))
        cur_time = int('%02d%02d' % (t.tm_hour, t.tm_min))
        if cur_time >= start_time and cur_time <= end_time:
            return True
    return False

def cache(website_id, video_id, fmt = '', url = '', cur_cache_dir = None):
    global active_video_list
    info({ 'code' : 'CACHE_THREAD_START', 'website_id' : website_id, 'video_id' : video_id, 'message' : 'Starting cache thread.' })
    eval('cache_' + website_id + '_video(website_id, video_id, fmt, url, cur_cache_dir)')
    video_key = video_queue.key(website_id, video_id, fmt)
    if video_key in active_video_list:
        active_video_list.remove(video_key)
    info({ 'code' : 'CACHE_THREAD_REMOVE', 'website_id' : website_id, 'video_id' : video_id, 'message' : 'Removed cache thread.' })

def schedule():
    global active_video_list
    while True:
        time.sleep(5)
        if not o.client_email:
            warn({ 'code' : 'RRE_LIAME_TNEILC'[::-1], 'message' : '.reludehcs-cv tratser ,oslA .diuqS tratser/daoler dna noitpo siht teS .tes ton si fnoc.ehcacoediv/cte/ ni liame_tneilc noitpo ehT'[::-1] })
            break
        try:
            if not is_caching_time(): continue
            if len(active_video_list) >= o.max_cache_processes: continue

            cur_cache_dir = get_next_cache_dir()
            if not cur_cache_dir: continue

            website_id, video_id, fmt, url = video_queue.get_popular()
            if not (website_id and video_id): continue

            video_key = video_queue.key(website_id, video_id, fmt)
            if video_key in active_video_list: continue
            active_video_list.append(video_key)
            cache_thread = LoggingThread(target = cache, name = video_key, args = (website_id, video_id, fmt, url, cur_cache_dir))
            cache_thread.start()
        except Exception, e:
            ent({ 'code' : 'VIDEO_SCHEDULE_ERR', 'message' : 'Could not schedule video for caching.', 'debug' : str(e) })

def purge_object(video_url):
    global this_opener
    if not this_opener:
        return False
    try:
        request = PurgeRequest(url = video_url)
        head_conn = this_opener.open(request)
        if 'getcode' in dir(head_conn) and head_conn.getcode() == 200:
            return True
        if 'code' in dir(head_conn) and head_conn.code == 200:
            return True
    except Exception, e:
        pass
    return False

def is_cached(video_url):
    global this_opener
    if not this_opener:
        return False
    try:
        request = HeadRequest(url = video_url, headers = { 'Cache-Control': 'only-if-cached' })
        head_conn = this_opener.open(request)
        if 'getcode' in dir(head_conn) and head_conn.getcode() == 200:
            return True
        if 'code' in dir(head_conn) and head_conn.code == 200:
            return True
    except Exception, e:
        if 'getcode' in dir(e) and e.getcode() == 504:
            return 2
        if 'code' in dir(e) and e.code == 504:
            return 2
        wnt({ 'code' : 'LOG_CHECK_ERR', 'message' : 'Could not confirm if url cached in Squid ' + video_url, 'debug' : str(e) })
    return False

def arg_drop_exception(website_id, params, arg):
    if website_id == 'youtube' and arg == 'begin' and params[arg][0] == '0':
        return True
    if website_id == 'hardsextube' and arg == 'start' and params[arg][0] == '0':
        return True
    if website_id == 'dailymotion' and arg == 'seek' and params[arg][0] == '0':
        return True
    if website_id == 'wrzuta' and arg == 'sec-offset' and params[arg][0] == '0':
        return True
    if website_id == 'redtube' and arg == 'ec_seek' and params[arg][0] == '0':
        return True
    if website_id == 'vimeo' and arg == 'aktimeoffset' and params[arg][0] == '0':
        return True
    if website_id == 'aol' and arg in ['set', 'timeoffset', 'aktimeoffset'] and params[arg][0] in ['0', '0.0']:
        return True
    if website_id == 'videobash' and arg == 'start' and params[arg][0] == '0':
        return True
    if website_id == 'xtube' and arg in ['start', 'fs', 'ms'] and params[arg][0] in ['0', '0.0']:
        return True
    return False

def watch_log_hit():
    global access_log_queue, youtube

    if not o.this_proxy_server:
        warn({ 'code': 'LOCAL_PROXY_NOT_SPECIFIED', 'message' : 'No value specified for option this_proxy in /etc/videocache.conf. Videocache will not be able to fetch videos from Squid cache to optimize bandwidth. Please specify a value for this_proxy option and restart vc-scheduler.' })
        return

    exp = 0
    while True:
        try:
            video_url = access_log_queue.pop()
            if not video_url:
                time.sleep(2 ** exp)
                exp = min_or_empty([exp + 1, 3])
                continue

            exp = 0
            fragments = urlparse.urlsplit(video_url)
            [host, path, query] = [fragments[1], fragments[2], fragments[3]]

            matched = False
            for website_id in o.enabled_website_keys:
                (matched, website_id, video_id, format, search, queue, report_hit) = eval('check_' + website_id + '_video(o, video_url, host, path, query)')
                if matched:
                    args = cgi.parse_qs(query)
                    for video_seek_arg in o.arg_drop_list[website_id]:
                        if video_seek_arg in args and not arg_drop_exception(website_id, args, video_seek_arg):
                            matched = False
                            break
                    break

            if not (matched and search):
                continue

            if website_id == 'youtube' and not is_valid_youtube_video_id(video_id):
                old_video_id = video_id
                cpn = get_youtube_cpn_from_query_or_path(query, path)
                if is_long_youtube_video_id(old_video_id): youtube.add_long_id(old_video_id, cpn)
                video_id = youtube.get_video_id(cpn, old_video_id)
                if video_id == None:
                    warn({ 'code' : 'MIGRATION_ISSUE', 'website_id' : website_id, 'message' : 'Youtube migration not fully effective in your country/region yet. Please ignore this issue if it doesnt occur for all the videos. URL ' + video_url })
                    continue

            if website_id != 'youtube' and not video_id:
                warn({ 'code' : 'VIDEO_ID_NOT_FOUND', 'website_id' : website_id, 'message' : 'Most probably you have set strip_query_terms to on in your squid.conf. Please set it to off and restart squid and vc-scheduler. URL ' + video_url })
                continue

            if not video_id:
                continue

            expected_size = None
            if website_id == 'youtube':
                youtube_range = get_youtube_video_range_from_query_or_path(query, path)
                expected_size = youtube_range['end'] - youtube_range['start']
                filename = get_youtube_filename(o, video_id, format, youtube_range)
            else:
                filename = eval('get_' + website_id + '_filename(o, video_id, format)')

            cached = False
            for directory in o.base_dirs[website_id]:
                if os.path.isfile(os.path.join(directory, filename)):
                    cached = True
                    break

            if cached:
                continue

            is_cached_result = is_cached(video_url)
            log_error = True
            if is_cached_result == True:
                info({ 'code' : 'LOG_HIT_CONFIRMED', 'video_id' : video_id, 'website_id' : website_id, 'message' : 'The video is present in local squid cache' })
            elif is_cached_result == 2 and website_id in o.log_hit_uncertain_exceptions:
                info({ 'code' : 'LOG_HIT_UNCERTAIN', 'video_id' : video_id, 'website_id' : website_id, 'message' : 'The video may be present in local squid cache. Will try anyway.' })
                log_error = False
            else:
                info({ 'code' : 'LOG_HIT_CONFIRM_FAILED', 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Either video is not present in squid cache anymore or the connection was closed before video buffer could complete.' })
                continue

            cache_dir = get_next_cache_dir()
            if not cache_dir:
                continue

            tmp_path = os.path.join(cache_dir, o.temp_dir, filename)
            video_path = os.path.join(cache_dir, o.website_cache_dir[website_id], filename)
            params = { 'website_id' : website_id, 'video_id' : video_id, 'http_headers' : {'X-Requested-With' : 'Videocache'}, 'proxy' : o.this_proxy_server, 'max_cache_speed' : 0, 'socket_read_block_size' : 2097152, 'log_error' : log_error }
            if website_id == 'android':
                params.update({ 'min_video_size' : o.min_android_app_size, 'max_video_size' : o.max_android_app_size })
            if timeout_exec(10, cache_remote_url, False, video_url, tmp_path, params):
                size = os.path.getsize(tmp_path)
                move_file(tmp_path, video_path)
                os.chmod(video_path, o.file_mode)
                os.utime(video_path, None)
                video_file.add_info(website_id, filename, cache_dir)
                info({ 'code' : 'VIDEO_CACHED', 'video_id' : video_id, 'website_id' : website_id, 'size' : size, 'message' : 'Video fetched from squid disk cache and stored at ' + video_path })
            else:
                if os.path.isfile(tmp_path): os.unlink(tmp_path)
        except Exception, e:
            ent({ 'code' : 'LOG_READ_ERR', 'message' : 'Error during reading message from log queue.', 'debug' : str(e) })
            time.sleep(0.05)

def update_disk_space():
    global base_dir_disk_space, cleanup_cache_needed
    for cache_dir in o.base_dir_list:
        space_available = free_space(cache_dir)
        space_used = partition_used(cache_dir)
        base_dir_disk_space[cache_dir] = { 'free_space' : space_available, 'use' : True }
        if space_used >= o.base_dir_thresholds[cache_dir]['high']:
            base_dir_disk_space[cache_dir].update({ 'use' : False })
            cleanup_cache_needed = True

def get_next_cache_dir():
    global last_base_dir_used, last_disk_space_update_at, disk_space_update_interval, sorted_disk_space, last_cache_dir_warn_at, cache_dir_warn_interval
    now = time.time()
    if now - last_disk_space_update_at > disk_space_update_interval:
        last_disk_space_update_at = now
        update_disk_space()
        if o.base_dir_selection == 3:
            vk = [(v['free_space'], k) for k, v in base_dir_disk_space.items()]
            sorted_disk_space = sorted(vk, reverse = True)

    if o.base_dir_selection == 1:
        for cache_dir in o.base_dir_list:
            if base_dir_disk_space[cache_dir]['use']:
                return cache_dir
    elif o.base_dir_selection == 2:
        for cache_dir in o.base_dir_list[(last_base_dir_used + 1):] + o.base_dir_list[:(last_base_dir_used + 1)]:
            if base_dir_disk_space[cache_dir]['use']:
                last_base_dir_used = o.base_dir_list.index(cache_dir)
                return cache_dir
    elif o.base_dir_selection == 3:
        for v, cache_dir in sorted_disk_space:
            if base_dir_disk_space[cache_dir]['use']:
                last_base_dir_used = o.base_dir_list.index(cache_dir)
                return cache_dir

    if now - last_cache_dir_warn_at > cache_dir_warn_interval:
        last_cache_dir_warn_at = now
        warn({'code' : 'CACHE_DIR_FULL', 'message' : 'All your cache directories have reached the disk availability threshold.'})
    return False

def need_rest_for():
    rest_for = 0.5
    try:
        load = os.getloadavg()[1]
        if load > 4.5:
            rest_for = 5
        elif load > 2.5:
            rest_for = 2
        else:
            rest_for = 0.1
    except Exception, e:
        pass
    return rest_for

def submit_video_info():
    try:
        if o.client_email != '':
            sys_info = { 'id' : o.id, 'email' : eval('o.cl' + 'ie' + 'nt_' + 'em' + 'ail'), 'version' : o.version, 'revision' : o.revision, 'trial' : o.trial }
            sys_info.update(get_all_info(o))
            new_info = {}
            for k,v in sys_info.items():
                new_info['[server][' + k + ']'] = v

            info_opener = urllib2

            status = info_opener.open(o.info_server, urllib.urlencode(new_info)).read()
        else:
            warn({ 'code' : 'RRE_LIAME_TNEILC'[::-1], 'message' : '.reludehcs-cv tratser ,oslA .diuqS tratser/daoler dna noitpo siht teS .tes ton si fnoc.ehcacoediv/cte/ ni liame_tneilc noitpo ehT'[::-1] })
    except Exception, e:
        wnt({ 'code' : 'SYNC_WARN', 'message' : 'Please report this if it occurs frequently.', 'debug' : str(e) })

def filelist_rebuild_required_for(cache_dir):
    global filelist_rebuild
    if filelist_rebuild[cache_dir]['rebuild'] or (time.time() - filelist_rebuild[cache_dir]['last_buid_at'] > o.filelist_rebuild_interval):
        return True
    return False

def filelist_rebuild_required():
    global filelist_rebuild
    for cache_dir in filelist_rebuild.keys():
        if filelist_rebuild_required_for(cache_dir):
            return True
    return False

def rebuild_filelist_for(cache_dir, website_id, cur_dir):
    letters = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-_#='
    os.chdir(cur_dir)
    total_files_created = 0
    total_files_updated = 0
    for l_1 in letters:
        for l_2 in letters:
            for filenames in chunks(glob.glob(l_1 + l_2 + '*'), 100):
                for filename in filenames:
                    video_key = video_file.key(website_id, filename)
                    if video_file.score_exists_for_key(video_key):
                        if not video_file.info_exists_for_key(video_key, cache_dir):
                            video_file.set_info_by_key(video_key, cache_dir)
                            total_files_updated += 1
                    else:
                        video_file.add_info(website_id, filename, cache_dir)
                        total_files_created += 1
                time.sleep(0.1)
        time.sleep(0.1)
    return total_files_created, total_files_updated

def rebuild_filelist():
    global filelist_rebuild
    for cache_dir in o.base_dir_list:
        if not filelist_rebuild_required_for(cache_dir): continue
        info({ 'code' : 'FILELIST_BUILD_START', 'message' : 'Starting filelist build for cache directory %s' % cache_dir })
        total_files_created, total_files_updated, started_at = 0, 0, time.time()
        for website_id in o.websites:
            cur_dir = os.path.join(cache_dir, o.website_cache_dir[website_id])
            if not os.path.isdir(cur_dir):
                create_dir(cur_dir, o.videocache_user)
                continue
            try:
                created, updated = rebuild_filelist_for(cache_dir, website_id, cur_dir)
                total_files_created += created
                total_files_updated += updated
            except Exception, e:
                ent({ 'code' : 'FILELIST_BUILD_FAIL', 'website_id' : website_id, 'message' : 'Failed to build filelist for cache_dir : ' + cache_dir, 'debug' : str(e) })
        info({ 'code' : 'FILELIST_BUILD_FINISH', 'message' : 'Created %s new entries and updated %s old entries in %s seconds for %s' % (total_files_created, total_files_updated, int(time.time() - started_at), cache_dir) })
        filelist_rebuild[cache_dir] = { 'last_buid_at' : int(time.time()), 'rebuild' : False }

def cleanup_video():
    try:
        delete_video(o, randomize)
    except Exception, e:
        wnt({ 'code' : 'SYNC_WARN', 'message' : 'Please report this if it occurs frequently.', 'debug' : str(e) })

def cleanup_tmp():
    now = time.time()
    for tmp_dir in o.base_dirs[o.temp_dir]:
        remove_count = 0
        for filenames in chunks(glob.glob(os.path.join(tmp_dir, '*')), 50):
            for filename in filenames:
                if os.path.isfile(filename) and (now - os.stat(filename).st_mtime) > o.tmp_file_lifetime:
                    if remove_file(filename): remove_count += 1
            time.sleep(0.5)
        if remove_count: info({ 'code' : 'TMP_DIR_CLEANUP', 'message' : 'Removed ' + str(remove_count) + ' orphaned files from ' + tmp_dir })
        time.sleep(0.5)

def thread_starter(thread, func, cur_time, last_run_at, run_interval):
    try:
        if thread and thread.is_alive():
            return thread, last_run_at
        elif cur_time - last_run_at > run_interval:
            thread = TaskThread(target = func, name = func.__name__)
            thread.start()
            return thread, cur_time
    except Exception, e:
        wnt({ 'code' : 'TASK_SCHEDULE_WARN', 'message' : 'Error in scheduling a task. Please report is this occurs frequently.', 'debug' : str(e) })
    return False, last_run_at

def task_scheduler():
    global cleanup_cache_needed
    now = time.time()
    refine_cache_periods()

    # cleanup_queues
    last_cq_at, cq_thread, cq_interval = 0, None, 180

    # cleanup_cache
    last_cc_at, cc_thread, cc_interval = now - 52, None, 40

    # rebuild_filelist
    last_rf_at, rf_thread, rf_interval = now - 55, None, 300

    # submit_video_info
    last_ssi_at, ssi_thread, ssi_interval = now - 21590, None, 21600

    # cleanup_video
    last_cv_at, cv_thread, cv_interval = now - 1780, None, 1800

    # log_hit_monitor
    last_hm_at, hm_thread, hm_interval = now - 165, None, 170

    # cleanup_tmp
    last_ct_at, ct_thread, ct_interval = now - 290, None, 300

    while True:
        cur_time = time.time()
        cq_thread, last_cq_at = thread_starter(cq_thread, cleanup_queues, cur_time, last_cq_at, cq_interval)
        ssi_thread, last_ssi_at = thread_starter(ssi_thread, submit_video_info, cur_time, last_ssi_at, ssi_interval)
        cv_thread, last_cv_at = thread_starter(cv_thread, cleanup_video, cur_time, last_cv_at, cv_interval)
        if cleanup_cache_needed:
            cc_thread, last_cc_at = thread_starter(cc_thread, cleanup_cache, cur_time, last_cc_at, cc_interval)
        hm_thread, last_hm_at = thread_starter(hm_thread, watch_log_hit, cur_time, last_hm_at, hm_interval)
        ct_thread, last_ct_at = thread_starter(ct_thread, cleanup_tmp, cur_time, last_ct_at, ct_interval)
        if filelist_rebuild_required():
            rf_thread, last_rf_at = thread_starter(rf_thread, rebuild_filelist, cur_time, last_rf_at, rf_interval)
        time.sleep(1)

def cleanup_cache(from_command_line = False):
    global video_file, cleanup_cache_needed, filelist_rebuild
    cleaner_info({ 'code' : 'CLEANER_START', 'message' : 'Videocache Cleaner invoked to cleanup old/unused videos to make space for the new ones' })
    if from_command_line: print 'Videocache Cleaner invoked to cleanup old/unused videos to make space for the new ones'

    for cache_dir in o.base_dir_list:
        try:
            while partition_used(cache_dir) > o.base_dir_thresholds[cache_dir]['low']:
                video_keys = video_file.get_least_used(2000)
                if len(video_keys) == 0:
                    # Trigger rebuild filelist if we can't find more to purge
                    if time.time() - filelist_rebuild[cache_dir]['last_buid_at'] > 21600 and not from_command_line:
                        filelist_rebuild[cache_dir]['rebuild'] = True
                    else:
                        cleaner_warn({ 'code' : 'CLEANER_OUT_OF_JUNK', 'message' : 'Videocache Cleaner could not find more files to remove from ' + cache_dir + '. Please check if any other program is consuming your disk space. Or tune your cache_swap_low and cache_swap_high options in /etc/videocache.conf accordingly.' })
                        if from_command_line:
                            print 'Videocache Cleaner could not find more files to remove from ' + cache_dir + '. Please check if any other program is consuming your disk space. Or tune your cache_swap_low and cache_swap_high options in /etc/videocache.conf accordingly.'
                        warn({ 'code' : 'CLEANER_OUT_OF_JUNK', 'message' : 'Videocache Cleaner could not find more files to remove from ' + cache_dir + '. Please check if any other program is consuming your disk space. Or tune your cache_swap_low and cache_swap_high options in /etc/videocache.conf accordingly.' })
                    break

                for chunk in chunks(video_keys, 100):
                    for video_key in chunk:
                        website_id, filename = video_file.decode_key(video_key)
                        if website_id and filename:
                            filepath = os.path.join(cache_dir, o.website_cache_dir[website_id], filename)
                            if os.path.isfile(filepath):
                                try:
                                    os.remove(filepath)
                                    cleaner_info({ 'code' : 'VIDEO_PURGED', 'message' : 'Removed ' + filepath, 'website_id' : website_id, 'video_id' : filename })
                                    if from_command_line: print 'Removed %s' % filepath
                                except Exception, e:
                                    cent({ 'code' : 'FILE_UNLINK_ERR', 'website_id' : website_id, 'message' : 'Error in removing file ' + filepath, 'debug' : str(e) })
                                    if from_command_line: print 'Error in removing file %s Error: %s' % (filepath, str(e))
                    video_file.remove_by_key(chunk)
                    if partition_used(cache_dir) < o.base_dir_thresholds[cache_dir]['low']: break
                    time.sleep(0.1)
                time.sleep(1)
        except Exception, e:
            cent({ 'code' : 'CLEANER_ERR', 'message' : 'Error occured while cleaning cache directory ' + cache_dir, 'debug' : str(e) })
            if from_command_line:
                print 'Error occured while cleaning cache directory ' + cache_dir, str(e)
                print traceback.format_exc()

    cleanup_cache_needed = False
    cleaner_info({ 'code' : 'CLEANER_FINISH', 'message' : 'Videocache Cleaner finished cleaning the old/unused videos and made space for the new ones' })
    if from_command_line:
        print 'Videocache Cleaner finished cleaning the old/unused videos and made space for the new ones'
    else:
        update_disk_space()

def cleanup_video_queue(from_command_line = False):
    global video_queue
    try:
        #info({ 'code' : 'VIDEO_QUEUE_CLEANUP', 'message' : 'Cleaning up video queue. Removing low scoring videos stuck in queue for long time' })
        if from_command_line: print 'Cleaning up video queue. Removing low scoring videos stuck in queue for long time'
        video_queue.expire_videos()
    except Exception, e:
        ent({ 'code' : 'VIDEO_QUEUE_CLEANUP_ERR', 'message' : 'Error while cleaning up video queue', 'debug' : str(e) })
        if from_command_line:
            print 'Error while cleaning up video queue'
            print traceback.format_exc()

def cleanup_youtube(from_command_line = False):
    global youtube
    try:
        #info({ 'code' : 'YOUTUBE_METADATA_CLEANUP', 'message' : 'Cleaning up stale Youtube metadata' })
        if from_command_line: print 'Cleaning up stale Youtube metadata'
        youtube.expire()
    except Exception, e:
        ent({ 'code' : 'YOUTUBE_METADATA_CLEANUP_ERR', 'message' : 'Error while cleaning up Youtube metadata', 'debug' : str(e) })
        if from_command_line:
            print 'Error while cleaning up Youtube metadata'
            print traceback.format_exc()

def cleanup_access_log_queue(from_command_line = False):
    global access_log_queue
    try:
        #info({ 'code' : 'ACCESS_LOG_QUEUE_CLEANUP', 'message' : 'Cleaning up url stuck in access log queue for long time' })
        if from_command_line: print 'Cleaning up url stuck in access log queue for long time'
        access_log_queue.trim()
    except Exception, e:
        ent({ 'code' : 'ACCESS_LOG_QUEUE_CLEANUP_ERR', 'message' : 'Error while cleaning up access log queue', 'debug' : str(e) })
        if from_command_line:
            print 'Error while cleaning up access log queue'
            print traceback.format_exc()

def cleanup_queues(from_command_line = False):
    cleanup_video_queue(from_command_line)
    cleanup_youtube(from_command_line)
    cleanup_access_log_queue(from_command_line)

def flush_video_queue(from_command_line = False):
    global video_queue
    try:
        if from_command_line: print 'Flushing videos queued for background caching'
        video_queue.flush()
    except Exception, e:
        ent({ 'code' : 'VIDEO_QUEUE_FLUSH_ERR', 'message' : 'Error while flushing videos queued for background caching', 'debug' : str(e) })
        if from_command_line:
            print 'Error while flushing videos queued for background caching'
            print traceback.format_exc()

def flush_youtube(from_command_line = False):
    global youtube
    try:
        if from_command_line: print 'Flushing Youtube metadata'
        youtube.flush()
    except Exception, e:
        ent({ 'code' : 'YOUTUBE_METADATA_FLUSH_ERR', 'message' : 'Error while flushing Youtube metadata', 'debug' : str(e) })
        if from_command_line:
            print 'Error while flushing Youtube metadata'
            print traceback.format_exc()

def flush_access_log_queue(from_command_line = False):
    global access_log_queue
    try:
        if from_command_line: print 'Flushing access log queue'
        access_log_queue.flush()
    except Exception, e:
        ent({ 'code' : 'ACCESS_LOG_QUEUE_FLUSH_ERR', 'message' : 'Error while flushing access log queue', 'debug' : str(e) })
        if from_command_line:
            print 'Error while flushing access log queue'
            print traceback.format_exc()

def flush_queues(from_command_line = False):
    flush_video_queue(from_command_line)
    flush_youtube(from_command_line)
    flush_access_log_queue(from_command_line)

class VideoPoolDaemon(VideocacheDaemon):

    def __init__(self, o = None, **kwargs):
        self.o = o
        VideocacheDaemon.__init__(self, '/var/run/videocache.pid', name = 'Videocache Scheduler', **kwargs)

    def run(self):
        global process_id
        try:
            self.o.set_loggers()
            process_id = os.getpid()

            task_scheduler()
        except Exception, e:
            ent({ 'code' : 'TASK_SCHEDULER_ERR', 'message' : 'Error while starting task scheduler.' })

if __name__ == '__main__':
    # Parse command line options.
    parser = OptionParser()
    parser.add_option('-s', '--signal', dest = 'sig', type='string', help = 'Send one of the following signals. start, stop, restart, kill')
    options, args = parser.parse_args()

    if options.sig:
        try:
            o = VideocacheOptions('/etc/videocache.conf', True)
        except Exception, e:
            message = 'Could not load Videocache configuration file. \nDebugging output: \n' + traceback.format_exc()
            syslog_msg(message.replace('\n', ''))
            sys.stderr.write(message)
            sys.exit(1)

        uid = None
        try:
            uid = pwd.getpwnam(o.videocache_user).pw_uid
        except Exception, e:
            pass
        if uid == None:
            message = 'Could not determine User ID for videocache user ' + o.videocache_user + '. \nDebugging output: \n' + traceback.format_exc()
            syslog_msg(message.replace('\n', ''))
            sys.stderr.write(message)
            sys.exit(1)

        daemon = VideoPoolDaemon(o, uid = uid)

        base_dir_disk_space = {}
        last_base_dir_used = 0
        last_disk_space_update_at = 0
        disk_space_update_interval = 120
        cleanup_cache_needed = False
        sorted_disk_space = None
        last_cache_dir_warn_at = 0
        cache_dir_warn_interval = 60
        exit = False
        process_id = '-'
        stdout = sys.stdout
        stderr = sys.stderr
        video_file = VideoFile(o)
        video_queue = VideoQueue(o)
        access_log_queue = AccessLogQueue(o)
        youtube = Youtube(o)
        active_video_list = []
        this_opener = None
        if o.this_proxy_server:
            this_opener = urllib2.build_opener(urllib2.ProxyHandler ({ 'http': o.this_proxy_server }))
        filelist_rebuild = {}
        for cache_dir in  o.base_dir_list:
            filelist_rebuild[cache_dir] = { 'last_buid_at' : 0, 'rebuild' : True }

        # Compiled regexes
        PID_EXTRACT_REGEX = re.compile('^[^\ ]+[ ]+([0-9]+)')

        randomize = int(random.random() * 10**8)

        # Import website functions
        for website_id in o.websites:
            exec(website_id + '_cached_url = generalized_cached_url')
            exec('get_' + website_id + '_filename = get_generalized_filename')
            exec('from websites.' + website_id + ' import *')

        if options.sig in ['flush', 'clean']:
            os.setgid(uid)
            os.setuid(uid)

        if options.sig == 'start':
            daemon.start()
        elif options.sig == 'stop':
            daemon.stop()
        elif options.sig == 'restart':
            daemon.restart()
        elif options.sig == 'status':
            daemon.status()
        elif options.sig == 'flush':
            flush_queues(True)
        elif options.sig == 'clean-queue':
            cleanup_queues(True)
        elif options.sig == 'clean':
            cleanup_cache(True)
        else:
            sys.stderr.write('Unknown signal received. See --help for more options.\n')
    else:
        parser.print_help()
        sys.stderr.write('\nNothing to do. Exit.\n')
        sys.exit(0)
