#!/usr/bin/env python
#
# (C) Copyright White Magnet Software Private Limited
# Company Website : http://whitemagnet.com/
# Product Website : http://cachevideos.com/
#

__author__ = """Kulbir Saini <saini@saini.co.in>"""
__docformat__ = 'plaintext'

from common import *
from database import initialize_database, VideoFile, VideoQueue, YoutubeCPN
from error_codes import *
from store import *
from vcdaemon import VideocacheDaemon
from vcoptions import VideocacheOptions
from vcsysinfo import *
from websites.youtube import *

from optparse import OptionParser
from Queue import Queue, Empty
from SimpleXMLRPCServer import SimpleXMLRPCServer
from xmlrpclib import ServerProxy

import cgi
import cookielib
import datetime
import glob
import heapq
import logging
import pickle
import pwd
import random
import shutil
import signal
import socket
import sys
import threading
import time
import traceback
import urllib2
import urlparse

# Cookie processor and default socket timeout
cj = cookielib.CookieJar()
urllib2.install_opener(urllib2.build_opener(urllib2.HTTPCookieProcessor(cj)))
socket.setdefaulttimeout(90)

def info(params = {}):
    if o.enable_scheduler_log:
        params.update({ 'logformat' : o.scheduler_logformat, 'timeformat' : o.timeformat, 'levelname' : logging.getLevelName(logging.INFO), 'process_id' : process_id })
        o.vcs_logger.info(build_message(params))

def error(params = {}):
    if o.enable_scheduler_log:
        params.update({ 'logformat' : o.scheduler_logformat, 'timeformat' : o.timeformat, 'levelname' : logging.getLevelName(logging.ERROR), 'process_id' : process_id })
        o.vcs_logger.error(build_message(params))

def warn(params = {}):
    if o.enable_scheduler_log:
        params.update({ 'logformat' : o.scheduler_logformat, 'timeformat' : o.timeformat, 'levelname' : logging.getLevelName(logging.WARN), 'process_id' : process_id })
        o.vcs_logger.debug(build_message(params))

def cleaner_info(params = {}):
    if o.enable_cleaner_log:
        params.update({ 'logformat' : o.cleaner_logformat, 'timeformat' : o.timeformat, 'levelname' : logging.getLevelName(logging.INFO), 'process_id' : process_id})
        o.vcc_logger.info(build_message(params))

def cleaner_error(params = {}):
    if o.enable_cleaner_log:
        params.update({ 'logformat' : o.cleaner_logformat, 'timeformat' : o.timeformat, 'levelname' : logging.getLevelName(logging.ERROR), 'process_id' : process_id})
        o.vcc_logger.error(build_message(params))

def cleaner_warn(params = {}):
    if o.enable_cleaner_log:
        params.update({ 'logformat' : o.cleaner_logformat, 'timeformat' : o.timeformat, 'levelname' : logging.getLevelName(logging.WARN), 'process_id' : process_id})
        o.vcc_logger.debug(build_message(params))

def trace(params = {}):
    if o.enable_trace_log:
        params.update({ 'logformat' : o.trace_logformat, 'timeformat' : o.timeformat, 'process_id' : process_id })
        o.trace_logger.info(build_message(params))

def ent(params = {}):
    error(params)
    params.update({ 'message' : traceback.format_exc() })
    trace(params)

def cent(params = {}):
    cleaner_error(params)
    params.update({ 'message' : traceback.format_exc() })
    trace(params)

def wnt(params = {}):
    error(params)
    params.update({ 'message' : traceback.format_exc() })
    trace(params)

def get_connection():
    global video_pool
    try:
        video_pool.ping()
        return video_pool
    except:
        pass

    try:
        video_pool = ServerProxy(o.rpc_url)
        video_pool.ping()
        return video_pool
    except Exception, e:
        ent({ 'code' : RPC_CONNECT_ERR, 'error_msg' : 'Could not connect to RPC server (videocache scheduler) at ' + o.rpc_host + ':' + str(o.rpc_port) + '. Please check scheduler status. If needed, restart scheduler using \'vc-scheduler -s restart\' command.', 'debug' : str(e) })
        return None

def connection():
    global video_pool
    try:
        video_pool.ping()
    except Exception, e:
        try:
            video_pool = ServerProxy(o.rpc_url)
            video_pool.ping()
        except Exception, e:
            try:
                ent({ 'code' : RPC_CONNECT_ERR, 'error_msg' : 'Could not connect to RPC server (videocache scheduler) at ' + o.rpc_host + ':' + str(o.rpc_port) + '. Please check scheduler status. If needed, restart scheduler using \'vc-scheduler -s restart\' command.', 'debug' : str(e) })
            except:
                pass

class VideoPool:
    scores_rebuild_threshold = 120
    systems = []
    hit_time_threshold = 15
    last_dump = 0
    dump_threshold = 120
    disk_space = {}
    last_used_dir = 0
    sync_lock = threading.Semaphore(value = 1)
    sync_threshold = 600
    store_log_threshold = 240
    cleanup_required = False

    scores = {}
    queue = {}
    active = {}
    scores_pq = {}
    scores_changed = {}
    last_scores_rebuild = {}
    last_sync = {}
    cpn_pool = {}
    subprocess_list = []

    def __init__(self):
        for website_id in o.websites:
            self.scores[website_id] = {}
            self.queue[website_id] = {}
            self.active[website_id] = {}
            self.scores_pq[website_id] = []
            self.scores_changed[website_id] = True
            self.last_scores_rebuild[website_id] = 0
            self.last_sync[website_id] = 0

    def ping(self):
        return True

    def is_cleanup_required(self):
        return self.cleanup_required

    def cleanup_finished(self):
        self.cleanup_required = False
        return True

    def add_system(self, new_system = {}):
        sys_info = { 'id' : o.id, 'email' : eval('o.cl' + 'ie' + 'nt_' + 'em' + 'ail'), 'version' : o.version, 'revision' : o.revision }
        sys_info.update(get_all_info(o))
        for system in [new_system, sys_info]:
            if system != {}:
                system.update({ 'un' : randomize })
                if system not in self.systems:
                    self.systems.append(system)
        return True

    def get_systems(self):
        self.add_system()
        return self.systems

    def load_queue(self):
        for dir in o.base_dir_list:
            queue_file = os.path.join(dir, o.queue_dump_file)
            if os.path.isfile(queue_file):
                try:
                    [scores, queue] = pickle.load(file(queue_file, 'r'))
                    if o.websites[0] not in scores or o.websites[0] not in queue:
                        self.flush()
                        info({ 'code' : QUEUE_INCOMPATIBLE, 'message' : 'Video queue flushed because it was incompatible with the current version' })
                        return True
                    # Different statement just in case the exception messes up assignment.
                    self.scores, self.queue = scores, queue
                    self.trim_queue()
                    info({ 'code' : QUEUE_LOAD, 'message' : 'Video queue loaded from dump file' })
                    return True
                except Exception, e:
                    wnt({ 'code' : QUEUE_LOAD_WARN, 'message' : 'Unable to load video queue from dump file at ' + queue_file, 'debug' : str(e) })
        return False

    def dump_queue(self, now = False):
        if now or (self.queue_changed_after_dump() and time.time() - self.last_dump > self.dump_threshold):
            self.last_dump = time.time()
            queue_dump_string = pickle.dumps([self.scores, self.queue])
            for dir in o.base_dir_list:
                queue_file = os.path.join(dir, o.queue_dump_file)
                try:
                    file(queue_file, 'w').write(queue_dump_string)
                except Exception, e:
                    wnt({ 'code' : QUEUE_DUMP_WARN, 'message' : 'Unable to dump video queue to dump file at ' + queue_file + '.', 'debug' : str(e) })
        return True

    def queue_changed_after_dump(self):
        for website_id in o.websites:
            if self.scores_changed[website_id] or self.last_scores_rebuild[website_id] > self.last_dump: return True
        return False

    def add_to_subprocess_list(self, pid):
        if pid not in self.subprocess_list:
            self.subprocess_list.append(pid)
        return True

    def remove_from_subprocess_list(self, pid):
        if pid in self.subprocess_list:
            self.subprocess_list.remove(pid)
        return True

    def stop_subprocesses(self):
        for pid in self.subprocess_list:
            try:
                os.kill(pid, signal.SIGTERM)
            except:
                pass
        return True

    def add_to_cpn_pool(self, video_id, cpn):
        if cpn not in self.cpn_pool:
            self.cpn_pool[cpn] = { 'video_id' : video_id, 'last_used' : time.time() }
        else:
            self.cpn_pool[cpn]['last_used'] = time.time()
        return True

    def get_youtube_video_id_from_cpn(self, cpn):
        if cpn in self.cpn_pool:
            self.cpn_pool[cpn]['last_used'] = time.time()
            if self.cpn_pool[cpn]['video_id']:
                return self.cpn_pool[cpn]['video_id']
        return False

    def get_cpn_pool(self):
        return self.cpn_pool

    def cleanup_cpn_pool(self):
        now = time.time()
        for cpn_id in self.cpn_pool.keys():
            if (now - self.cpn_pool[cpn_id]['last_used']) > o.cpn_lifetime:
                self.cpn_pool.pop(cpn_id, None)
        return True

    def add_videos(self, videos = {}):
        info({ 'code' : VIDEOS_RECEIVED, 'message' : 'Received ' + str(len(videos)) + ' videos from Videocache.' })

        self.trim_queue()

        self.sync_lock.acquire()
        for video_id in videos:
            for params in videos[video_id]:
                self.add_video(video_id, params)
        self.sync_lock.release()
        return True

    def is_uncacheable_youtube(self, video_id, website_id):
        if video_id and len(video_id) != 11 and website_id == 'youtube': return True
        return False

    def remove_youtube_video_format(self, video_id, website_id, format):
        if video_id in self.queue[website_id] and format in self.queue[website_id][video_id]['fmts']:
            self.queue[website_id][video_id]['fmts'].pop(format)
        return True

    def add_youtube_video(self, video_id, website_id, params):
        fmt = params['format']
        params['urls'] = []
        if video_id not in self.queue[website_id]:
            if fmt:
                params['fmts'] = { fmt : { 'score' : 1, 'access_time' : params['access_time'] } }
            else:
                params['fmts'] = {}
            self.queue[website_id][video_id] = params
            self.queue[website_id][video_id]['first_access'] = params['access_time']
            self.set_score(video_id, website_id, 1)
        elif fmt:
            old_data = self.queue[website_id][video_id]
            if old_data['fmts'].has_key(fmt):
                if old_data['client_ip'] == params['client_ip'] and (params['access_time'] - old_data['fmts'][fmt]['access_time'] > self.hit_time_threshold):
                    self.inc_score(video_id, website_id, 1)
                    self.queue[website_id][video_id]['fmts'][fmt].update({ 'score' : old_data['fmts'][fmt].get('score', 0) + 1, 'access_time' : params['access_time'] })
            else:
                self.queue[website_id][video_id]['fmts'][fmt] = { 'score' : 1, 'access_time' : params['access_time'] }
        return True

    def add_video(self, video_id, params):
        website_id = params['website_id']
        if website_id == 'youtube':
            params['urls'] = []
            return self.add_youtube_video(video_id, website_id, params)

        if video_id not in self.queue[website_id]:
            self.queue[website_id][video_id] = params
            self.queue[website_id][video_id]['first_access'] = params['access_time']
            self.set_score(video_id, website_id, 1)
        else:
            old_data = self.queue[website_id][video_id]
            try:
                if params['client_ip'] == old_data['client_ip'] and (int(params['access_time']) - int(old_data['access_time'])) > self.hit_time_threshold:
                    self.inc_score(video_id, website_id)
                    self.queue[website_id][video_id].update({ 'access_time' : params['access_time'] })
            except Exception, e:
                self.inc_score(video_id, website_id)
        return True

    def trim_queue(self):
        videos_in_queue = self.get_queue_length()
        if videos_in_queue - o.max_cache_queue_size >= 0:
            self.remove_least_popular(videos_in_queue - o.max_cache_queue_size + 1)
        return True

    def sync_queue(self, website_id = None, sync_now = False):
        if website_id:
            if sync_now or (time.time() - self.last_sync[website_id]) > self.sync_threshold:
                self.sync_lock.acquire()
                new_queue = {}
                for video_id in self.scores[website_id].keys():
                    if video_id in self.queue[website_id]:
                        new_queue[video_id] = self.queue[website_id][video_id]
                self.queue[website_id] = new_queue
                self.last_sync[website_id] = time.time()
                self.sync_lock.release()
        else:
            [self.sync_queue(web_id, sync_now) for web_id in o.websites]
        return True

    def rebuild_scores_pq(self, website_id = None, rebuild_scores_now = False):
        if website_id:
            #FIXME AND vs OR when to rebuild scores priority queue
            #if self.scores_changed[website_id] and (rebuild_scores_now or time.time() - self.last_scores_rebuild[website_id] > self.scores_rebuild_threshold):
            #FIXME rebuilding scores makes sense only if they have changed.
            #if self.scores_changed[website_id] or (rebuild_scores_now or time.time() - self.last_scores_rebuild[website_id] > self.scores_rebuild_threshold):
            if rebuild_scores_now or self.scores_changed[website_id]:
                self.scores_pq[website_id] = [(v,k) for k,v in self.scores[website_id].items()]
                self.scores_changed[website_id], self.last_scores_rebuild[website_id] = False, time.time()
        else:
            [self.rebuild_scores_pq(web_id, rebuild_scores_now) for web_id in o.websites]
        return True

    def get_score(self, video_id, website_id):
        """Get the score of video represented by video_id."""
        if website_id in self.scores:
            return self.scores[website_id].get(video_id, 0)
        return 0

    def set_score(self, video_id, website_id, score = 1):
        """Set the priority score of a video_id."""
        if website_id in self.scores:
            self.scores[website_id][video_id] = score
            self.scores_changed[website_id] = True
        return True

    def inc_score(self, video_id, website_id, incr = 1):
        """Increase the priority score of video represented by video_id."""
        if website_id in self.scores and video_id in self.scores[website_id]:
            self.scores[website_id][video_id] += incr
            self.scores_changed[website_id] = True
            return True
        return False

    def dec_score(self, video_id, website_id, decr = 1):
        """Decrease the priority score of video represented by video_id."""
        if website_id in self.scores and video_id in self.scores[website_id]:
            self.scores[website_id][video_id] -= decr
            self.scores_changed[website_id] = True
            return True
        return False

    def get_popular(self, count = 1, website_id = None):
        """Return the video_id of the most frequently access video."""
        self.sync_queue(website_id)
        self.rebuild_scores_pq(website_id)

        if website_id:
            return [(video_id, website_id) for score, video_id in heapq.nlargest(count, self.scores_pq[website_id])]
        else:
            return [(v, w) for s, v, w in heapq.nlargest(count, [(self.get_score(vid, wid), vid, wid) for vid, wid in [hit for hit_list in [self.get_popular(count, web_id) for web_id in o.websites] for hit in hit_list]])]

    def get_least_popular(self, count = 1, website_id = None):
        """Get `count` number of least popular videos"""
        self.rebuild_scores_pq(website_id)

        if website_id:
            return [(video_id, website_id) for score, video_id in heapq.nsmallest(count, self.scores_pq[website_id])]
        else:
            return [(v, w) for s, v, w in heapq.nsmallest(count, [(self.get_score(vid, wid), vid, wid) for vid, wid in [flop for flop_list in [self.get_least_popular(count, web_id) for web_id in o.websites] for flop in flop_list]])]

    def remove_least_popular(self, count = 1, website_id = None):
        """Remove `count` number of least popular videos fromt the queue."""
        for video_id, web_id in self.get_least_popular(count, website_id):
            self.remove(video_id, web_id)
        return True

    def get_details(self, video_id, website_id):
        """Return the details of a particular video represented by video_id."""
        if website_id in self.queue:
            return self.queue[website_id].get(video_id, { 'fmts' : {} })
        return { 'fmts' : {} }

    def remove_from_queue(self, video_id, website_id):
        """Dequeue a video_id from the download queue."""
        item = (self.get_score(video_id, website_id), video_id)
        item in self.scores_pq[website_id] and self.scores_pq[website_id].remove(item)
        self.scores_changed[website_id] = True
        video_id in self.queue[website_id] and self.queue[website_id].pop(video_id)
        video_id in self.scores[website_id] and self.scores[website_id].pop(video_id)
        return True

    def remove_url_from_queue(self, video_id, website_id, url):
        """Dequeue a url for a video_id from the download queue."""
        video_id in self.queue[website_id] and url in self.queue[website_id][video_id]['urls'] and self.queue[website_id][video_id]['urls'].remove(url)
        return True

    def remove(self, video_id, website_id):
        """Remove video_id from queue as well as active connection list."""
        return self.remove_from_queue(video_id, website_id) and self.remove_conn(video_id, website_id)

    def remove_url(self, video_id, website_id, url):
        """Remove url from url list for a video_id."""
        if len(self.queue[website_id][video_id]['urls']) == 1:
            return self.remove(video_id, website_id)
        else:
            return self.remove_url_from_queue(video_id, website_id, url)

    def flush(self, website_id = None):
        """Flush the queue and reinitialize everything."""
        if website_id:
            self.scores[website_id] = {}
            self.queue[website_id] = {}
            self.active[website_id] = {}
            self.scores_pq[website_id] = []
            self.scores_changed[website_id] = True
            self.last_scores_rebuild[website_id] = 0
            self.last_sync[website_id] = 0
            return True
        else:
            [self.flush(web_id) for web_id in o.websites]
        self.dump_queue(True)
        return True

    def get_queue_length(self, website_id = None):
        if website_id:
            return len(self.scores[website_id])
        else:
            return sum([self.get_queue_length(web_id) for web_id in o.websites])

    def get_pq_length(self, website_id = None):
        if website_id:
            return len(self.scores_pq[website_id])
        else:
            return sum([self.get_pq_length(web_id) for web_id in o.websites])

    def get_active(self, website_id = None):
        if website_id:
            return self.active[website_id]
        else:
            return self.active

    def get_scores(self, website_id = None):
        if website_id:
            return self.scores[website_id]
        else:
            return self.scores

    def get_queue(self, website_id = None):
        if website_id:
            return self.queue[website_id]
        else:
            return self.queue

    def get_scores_pq(self, website_id = None):
        if website_id:
            return self.scores_pq[website_id]
        else:
            return self.scores_pq

    def update_disk_space(self):
        for dir in o.base_dir_list:
            space_available = free_space(dir)
            self.disk_space[dir] = { 'free_space' : space_available, 'use' : True }
            if space_available < o.disk_avail_threshold:
                self.disk_space[dir].update({ 'use' : False })
        self.cleanup_required = all(map(lambda x: not self.disk_space[x]['use'], self.disk_space))

    def get_next_cache_dir(self):
        self.update_disk_space()
        if o.base_dir_selection == 1:
            for dir in o.base_dir_list:
                if self.disk_space[dir]['use']:
                    return dir
        elif o.base_dir_selection == 2:
            for dir in o.base_dir_list[(self.last_used_dir + 1):] + o.base_dir_list[:(self.last_used_dir + 1)]:
                if self.disk_space[dir]['use']:
                    self.last_used_dir = o.base_dir_list.index(dir)
                    return dir
        elif o.base_dir_selection == 3:
            vk = [(v['free_space'], k) for k, v in self.disk_space.items()]
            for v, dir in sorted(vk, reverse = True):
                if self.disk_space[dir]['use']:
                    self.last_used_dir = o.base_dir_list.index(dir)
                    return dir

        warn({'code' : CACHE_DIR_FULL, 'message' : 'All your cache directories have reached the disk availability threshold.'})
        return False

    def start_cache_thread(self, video_id, website_id, params):
        """Start a new thread which will cache the vdieo"""
        new_thread = threading.Thread(target = cache, name = str(video_id), kwargs = { 'params' : params })
        self.set_score(video_id, website_id, 0)
        self.add_conn(video_id, website_id, new_thread)
        new_thread.start()
        info({ 'code' : CACHE_THREAD_START, 'website_id' : website_id, 'video_id' : video_id, 'message' : 'Starting cache thread.' })
        return True

    def clean_threads(self, website_id = None):
        """Cleanup threads which have exitted."""
        if website_id:
            for (video_id, thread) in self.active[website_id].items():
                try:
                    if not thread or not thread.isAlive():
                        self.remove(video_id, website_id)
                        info({ 'code' : CACHE_THREAD_REMOVE, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Cache thread completed. Removing from active list.' })
                except Exception, e:
                    ent({ 'code' : CACHE_THREAD_REMOVE_ERR, 'website_id' : website_id, 'message' : 'Unable to remove cache thread from thread list', 'debug' : str(e), 'video_id' : video_id })
        else:
            [self.clean_threads(web_id) for web_id in o.websites]
        return True

    def schedule(self):
        try:
            self.clean_threads()
            self.dump_queue()
            self.cleanup_cpn_pool()
            if o.offline_mode or not is_caching_time():
                return True
            if self.get_conn_number() < o.max_cache_processes:
                popular = self.get_popular()
                if len(popular) != 1 or len(popular[0]) != 2:
                    return True
                video_id, website_id = popular[0]
                if self.is_uncacheable_youtube(video_id, website_id):
                    self.set_score(video_id, website_id, max(self.get_score(video_id, website_id) - 1, 0))
                    return True
                if video_id != False and self.is_active(video_id, website_id) == False and self.get_score(video_id, website_id) >= o.hit_threshold:
                    params = self.get_details(video_id, website_id)
                    if params != False:
                        if o.enable_store_log_monitoring and (time.time() - params.get('first_access', 0)) < self.store_log_threshold:
                            return True
                        next_cache_dir = self.get_next_cache_dir()
                        if next_cache_dir is False:
                            return True
                        else:
                            params.update({ 'cur_cache_dir' : next_cache_dir })
                        self.start_cache_thread(video_id, website_id, params)
                        return True
                elif self.is_active(video_id, website_id) == True:
                    self.set_score(video_id, website_id, 0)
                    return True
        except Exception, e:
            ent({ 'code' : VIDEO_SCHEDULE_ERR, 'message' : 'Could not find out the next video to schedule.', 'debug' : str(e) })
            return False
        return True

    # Functions related download scheduling.
    # Have to mess up things in single class because python
    # RPCServer doesn't allow to register multiple instances.
    def add_conn(self, video_id, website_id, thread):
        """Add video_id to active connections list."""
        if video_id not in self.active[website_id]:
            self.active[website_id][video_id] = thread
        return True

    def set_conn(self, video_id, website_id, thread):
        self.active[website_id][video_id] = thread
        return True

    def get_conn_number(self, website_id = None):
        """Return the number of currently active connections."""
        if website_id:
            return len(self.active[website_id])
        else:
            return sum([self.get_conn_number(web_id) for web_id in o.websites])

    def is_active(self, video_id, website_id):
        """Returns whether a connection is active or not."""
        return video_id in self.active[website_id]

    def remove_conn(self, video_id, website_id):
        """Remove video_id from active connections list."""
        if video_id in self.active[website_id]:
            self.active[website_id].pop(video_id)
        return True

class VideoPoolRPCServer(SimpleXMLRPCServer):

    allow_reuse_address = True

    def __init__(self, *args, **kwargs):
        self.finished = False
        SimpleXMLRPCServer.__init__(self, *args, **kwargs)

    def shutdown(self):
        self.finished = True

    def server_bind(self):
        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.socket.bind(self.server_address)

    def serve_forever(self):
        while not self.finished:
            self.handle_request()

def cache(params):
    """This function caches the remote file."""
    video_id = params.get('video_id', '-')
    website_id = params.get('website_id', '-')
    try:
        eval('cache_' + website_id + '_video(params)')
    except Exception, e:
        ent({'code' : CACHE_THREAD_ERR, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Error in cache thread.'})

def get_random_user_agent():
    return o.std_headers[random.randint(0, o.num_std_headers - 1)]

def cache_remote_url(remote_url, target_file, params = {}):
    video_id = params.get('video_id', '-')
    website_id = params.get('website_id', '-')
    http_headers = params.get('http_headers', {})
    proxy = params.get('proxy', o.proxy)
    max_cache_speed = params.get('max_cache_speed', o.max_cache_speed)
    min_video_size = params.get('min_video_size', o.min_video_size)
    max_video_size = params.get('max_video_size', o.max_video_size)

    if not http_headers.has_key('User-Agent'):
        http_headers.update(get_random_user_agent())

    if proxy:
        urllib2.install_opener(urllib2.build_opener(urllib2.ProxyHandler({ 'http' : proxy, 'https' : proxy, 'ftp' : proxy })))
    request = urllib2.Request(remote_url, None, http_headers)

    try:
        connection = urllib2.urlopen(request)
        try:
            conn_info = connection.info()
            video_size = int(conn_info.get('content-length', -1))
            if video_size == -1:
                if o.force_video_size == 1:
                    info({ 'code' : VIDEO_SIZE_NOT_SUPPLIED, 'message' : 'Video size was not supplied by web server. Skipping.', 'video_id' : video_id, 'website_id' : website_id })
                    return False
            else:
                if max_video_size != 0 and video_size > max_video_size:
                    info({ 'code' : VIDEO_TOO_LARGE, 'message' : 'Video size is large than the specified max video size allowed. Skipping.', 'video_id' : video_id, 'website_id' : website_id, 'size' : video_size })
                    return False
                if min_video_size != 0 and video_size < min_video_size:
                    info({ 'code' : VIDEO_TOO_SMALL, 'message' : 'Video size is smaller than the specified min video size allowed. Skipping.', 'video_id' : video_id, 'website_id' : website_id, 'size' : video_size })
                    return False
        except Exception, e:
            wnt({ 'code' : CONNECTION_INFO_ERR, 'message' : 'Error while getting connection information.', 'debug' : str(e), 'video_id' : video_id, 'website_id' : website_id })

        file = None
        start_time = time.time()
        downloaded = 0
        while True:
            block = connection.read(16384)
            if len(block) == 0:
                break
            if not file:
                file = open(target_file, 'wb')
            file.write(block)

            downloaded += len(block)
            while max_cache_speed > 0 and downloaded / (time.time() - start_time) > max_cache_speed:
                time.sleep(0.2)
        if file:
            file.close()

        target_file_size = os.path.getsize(target_file)
        if video_size != -1 and target_file_size < 0.97 * video_size:
            error({ 'code' : PARTIAL_CACHE_ERR, 'message' : 'Video could not be cached completely. Expected: ' + str(video_size) + ', Got: ' + str(target_file_size), 'video_id' : video_id, 'website_id' : website_id })
            os.unlink(target_file)
            return False
    except urllib2.HTTPError, e:
        try:
            ent({ 'code' : CACHE_HTTP_ERR, 'message' : 'HTTP error : ' + str(e.code) + '. An error occured while caching the video at '  + remote_url + '.', 'debug' : str(e), 'video_id' : video_id, 'website_id' : website_id })
            return False
        except:
            ent({ 'code' : CACHE_HTTP_ERR, 'message' : 'HTTP error. An error occured while caching the video at '  + remote_url + '.', 'debug' : str(e), 'video_id' : video_id, 'website_id' : website_id })
            return False
    except Exception, e:
        ent({ 'code' : CACHE_ERR, 'message' : 'Could not cache the video at ' + remote_url + '.', 'debug' : str(e), 'video_id' : video_id, 'website_id' : website_id })
        return False
    return True

def migrate_to_new_youtube_video_ids(website_id, old_video_id, new_video_id):
    if not old_video_id or not new_video_id:
        return
    for youtube_dir in o.base_dirs[website_id]:
        for filename in glob.glob(os.path.join(youtube_dir, old_video_id + '*')):
            try:
                shutil.move(filename, filename.replace(old_video_id, new_video_id))
            except:
                pass

def cache_youtube_video(params):
    video_id = params.get('video_id', '-')
    website_id = params.get('website_id', '-')
    format = params.get('format', '')
    cur_cache_dir = params.get('cur_cache_dir', None)
    proxy = params.get('proxy', o.proxy)
    http_headers = get_random_user_agent()
    params['http_headers'] = http_headers

    if video_id == '-' or website_id == '-' or cur_cache_dir == None:
        error({ 'code' : VIDEO_INFO2_ERR, 'website_id' : website_id, 'video_id' : video_id, 'message' : 'Enough video info not available in thread' })
        return

    if proxy: urllib2.install_opener(urllib2.build_opener(urllib2.ProxyHandler({ 'http' : proxy, 'https' : proxy, 'ftp' : proxy })))

    video_info = None
    for el in ['&el=detailpage', '&el=embedded', '&el=vevo', '']:
        info_url = 'http://www.youtube.com/get_video_info?video_id=%s%s&ps=default&eurl=&gl=US&hl=en' % (video_id, el)
        request = urllib2.Request(info_url, None, http_headers)
        try:
            video_info = cgi.parse_qs(urllib2.urlopen(request).read())
            try:
                view_count = int(video_info.get('view_count', [0])[0])
            except Exception, e:
                view_count = o.min_youtube_views + 100

            if view_count < o.min_youtube_views:
                info({ 'code' : VIEW_COUNT_LOW, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Video has not received enough views and will not be cached.' })
                return
            if 'url_encoded_fmt_stream_map' in video_info:
                break
        except Exception, e:
            ent({'code' : VIDEO_INFO_FETCH_ERR, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Error occured while fetching video info', 'debug' : str(e) })
    else:
        error({ 'code' : VIDEO_INFO_FETCH_ERR, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Could not fetch required info to cache video' })
        return

    try:
        url_map = {}
        [url_map.update({i['itag'][0] : i['url'][0] + '&signature=' + i['sig'][0]}) for i in [cgi.parse_qs(i) for i in video_info['url_encoded_fmt_stream_map'][0].split(',')]]
        alternate_vid = get_youtube_video_id(url_map.values()[0])
        if alternate_vid and len(alternate_vid) == 16:
            migrate_to_new_youtube_video_ids(website_id, alternate_vid, video_id)
    except Exception, e:
        ent({ 'code' : URL_EXTRACTION_ERROR, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Error while extracting video urls.', 'debug' : str(e) })
        return

    cache_dir = os.path.join(cur_cache_dir, o.website_cache_dir[website_id])
    tmp_dir = os.path.join(cur_cache_dir, o.temp_dir)

    video_details = video_pool.get_details(video_id, website_id)['fmts']
    if alternate_vid:
        video_details_2 = video_pool.get_details(alternate_vid, website_id)['fmts']
        if len(video_details_2.keys()) > 0:
            all_keys = list(set(video_details.keys() + video_details_2.keys()))
            new_details = {}
            for vd in [video_details, video_details_2]:
                for fmt_key in vd.keys():
                    new_details[fmt_key] = { 'score' : vd[fmt_key]['score'] + new_details.get(fmt_key, { 'score' : 0 })['score'] }
            video_details = new_details

    fmt_scores = sorted([(video_details[itag]['score'], itag) for itag in video_details], reverse = True)
    if len(fmt_scores) == 0:
        fmt_scores = [(2, '35')]

    first = True
    num_cached_formats = 0
    for score, fmt in fmt_scores:
        if first:
            first = False
        else:
            popular = video_pool.get_popular(1)
            if len(popular) != 1 and len(popular[0]) != 2:
                continue
            pop_video_id, pop_web_id = popular[0]
            if score < video_pool.get_score(pop_video_id, pop_web_id):
                video_pool.set_score(video_id, website_id, score)
                video_pool.remove_conn(video_id, website_id)
                break

        if url_map.has_key(fmt):
            if o.youtube_formats.has_key(fmt) and o.youtube_formats[fmt]['res'] > o.max_youtube_video_quality:
                continue
            if o.enable_youtube_html5_videos == 0 and o.youtube_formats[fmt]['cat'] in ['webm', 'webm_3d']:
                continue
            if o.enable_youtube_3d_videos == 0 and o.youtube_formats[fmt]['cat'] in ['regular_3d', 'webm_3d']:
                continue
            if youtube_cached_url(o, alternate_vid, website_id, fmt)[0]:
                filename = get_youtube_filename(o, alternate_vid, fmt)
                new_filename = get_youtube_filename(o, video_id, fmt)
                video_path = os.path.join(cache_dir, filename)
                new_video_path = os.path.join(cache_dir, new_filename)
                info({ 'code' : VIDEO_EXISTS, 'website_id' : website_id, 'video_id' : alternate_vid, 'message' : 'FORMAT ' + fmt + ' Video already exists at ' + video_path + '.' })
                video_pool.remove_youtube_video_format(video_id, website_id, fmt)
                if alternate_vid: video_pool.remove_youtube_video_format(alternate_vid, website_id, fmt)
                num_cached_formats += 1
                if os.path.isfile(video_path) and not os.path.isfile(new_video_path):
                    shutil.move(video_path, new_video_path)
                    info({ 'code' : 'VIDEO_MOVED', 'website_id' : website_id, 'video_id' : alternate_vid, 'message' : 'Video ' + video_path + ' moved to ' + new_video_path + '.' })
            elif youtube_cached_url(o, video_id, website_id, fmt)[0]:
                info({ 'code' : VIDEO_EXISTS, 'website_id' : website_id, 'video_id' : video_id, 'message' : 'FORMAT ' + fmt + ' Video already exists at ' + video_path + '.' })
                video_pool.remove_youtube_video_format(video_id, website_id, fmt)
                if alternate_vid: video_pool.remove_youtube_video_format(alternate_vid, website_id, fmt)
                num_cached_formats += 1
            else:
                video_url = url_map[fmt]
                filename = get_youtube_filename(o, video_id, fmt)
                video_path = os.path.join(cache_dir, filename)
                tmp_path = os.path.join(tmp_dir, filename)

                try:
                    if cache_remote_url(video_url, tmp_path, params):
                        size = os.path.getsize(tmp_path)
                        shutil.move(tmp_path, video_path)
                        os.chmod(video_path, o.file_mode)
                        os.utime(video_path, None)
                        info({ 'code' : VIDEO_CACHED, 'video_id' : video_id, 'website_id' : website_id, 'size' : size, 'message' : 'FORMAT ' + fmt + ' Video was cached successfully at ' + video_path })
                        num_cached_formats += 1
                        video_pool.remove_youtube_video_format(video_id, website_id, fmt)
                        if alternate_vid: video_pool.remove_youtube_video_format(alternate_vid, website_id, fmt)
                        if o.use_db: VideoFile.create({ 'cache_dir' : cur_cache_dir, 'website_id' : website_id, 'filename' : filename, 'size' : size, 'access_time' : current_time() })
                    else:
                        if os.path.isfile(tmp_path): os.unlink(tmp_path)
                except Exception, e:
                    ent({ 'code' : URL_CACHE_ERR, 'video_id' : video_id, 'message' : 'Failed to cache video at ' + video_url + '.', 'website_id' : website_id })
        else:
            info({ 'code' : 'UNSUPPORTED_FORMAT', 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Background caching of format ' + str(fmt) + ' is not supported' })
            video_pool.remove_youtube_video_format(video_id, website_id, fmt)
            if alternate_vid: video_pool.remove_youtube_video_format(alternate_vid, website_id, fmt)

    video_details = video_pool.get_details(video_id, website_id)
    if len(video_details['fmts'].keys()) == 0 or len(video_details['fmts'].keys()) == num_cached_formats:
        video_pool.remove(video_id, website_id)
        if alternate_vid: video_pool.remove(alternate_vid, website_id)
        info({ 'code' : ALL_QUEUED_FORMATS_CACHED, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'All formats for this video has been cached already.' })

def cache_dailymotion_video(params):
    video_id = params.get('video_id', '-')
    website_id = params.get('website_id', '-')
    format = params.get('format', '')
    urls = params.get('urls', [])
    cur_cache_dir = params.get('cur_cache_dir', None)
    proxy = params.get('proxy', o.proxy)
    http_headers = get_random_user_agent()
    params['http_headers'] = http_headers

    if video_id == '-' or website_id == '-' or cur_cache_dir == None or len(urls) == 0:
        error({ 'code' : VIDEO_INFO2_ERR, 'message' : 'Enough video information was not available in cache thread.', 'website_id' : website_id, 'video_id' : video_id })
        return

    video_url = None
    alternate_vid = None

    if proxy:
        urllib2.install_opener(urllib2.build_opener(urllib2.ProxyHandler({ 'http' : proxy, 'https' : proxy, 'ftp' : proxy })))

    try:
        request = urllib2.Request('http://www.dailymotion.com/family_filter?enable=false', None, http_headers)
        urllib2.urlopen(request).read()
    except Exception, e:
        wnt({ 'code' : DAILYMOTION_FF_ERR, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Could not disable family filter for dailymotion.com.', 'debug' : str(e) })

    request = urllib2.Request(urls[0], None, http_headers)

    try:
        page_data = urllib2.urlopen(request).read()
        if page_data:
            page_data = urllib2.unquote(page_data).replace('\\/', '/')
            search_sdurl = re.compile('"sdURL":"([^"]*)"').search(page_data)
            if search_sdurl:
                video_url = search_sdurl.group(1)
            else:
                search_hqurl = re.compile('"hqURL":"([^"]*)"').search(page_data)
                if search_hqurl:
                    video_url = search_hqurl.group(1)

        if video_url:
            request = urllib2.Request(video_url, None, http_headers)
            webpage = urllib2.urlopen(request)
            if webpage and webpage.url != '':
                a, b, alternate_vid, format, d, e = check_dailymotion_video(webpage.url)
                if generalized_cached_url(o, alternate_vid, website_id, format)[0]:
                    info({ 'code' : VIDEO_EXISTS, 'message' : 'ALTERNATE_VIDEO_ID ' + alternate_vid + ' Video already exists', 'website_id' : website_id, 'video_id' : video_id })
                    return

    except Exception, e:
        ent({ 'code' : VIDEO_PAGE_ERR, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Error while fetching video webpage.', 'debug' : str(e) })
        return

    if not video_url:
        error({ 'code' : VIDEO_URL_ERR, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'Could not determine video URL.' })
        return

    cache_dir = os.path.join(cur_cache_dir, o.website_cache_dir[website_id])
    tmp_dir = os.path.join(cur_cache_dir, o.temp_dir)

    video_path = os.path.join(cache_dir, alternate_vid)
    tmp_path = os.path.join(tmp_dir, alternate_vid)

    try:
        if not os.path.exists(video_path):
            if cache_remote_url(video_url, tmp_path, params):
                size = os.path.getsize(tmp_path)
                shutil.move(tmp_path, video_path)
                os.chmod(video_path, o.file_mode)
                os.utime(video_path, None)
                info({ 'code' : VIDEO_CACHED, 'video_id' : video_id, 'website_id' : website_id, 'size' : size, 'message' : 'ALTERNATE_VIDEO_ID ' + alternate_vid + ' Video was cached successfully at ' + video_path })
                if o.use_db: VideoFile.create({ 'cache_dir' : cur_cache_dir, 'website_id' : website_id, 'filename' : alternate_vid, 'size' : size, 'access_time' : current_time() })
            else:
                if os.path.isfile(tmp_path): os.unlink(tmp_path)
                return
        else:
            info({ 'code' : VIDEO_EXISTS, 'website_id' : website_id, 'video_id' : video_id, 'message' : 'ALTERNATE_VIDEO_ID ' + alternate_vid + ' Video already exists at ' + video_path })

        return
    except Exception, e:
        ent({ 'code' : URL_CACHE_ERR, 'website_id' : website_id, 'video_id' : video_id, 'message' : 'Failed to cache video at ' + video_url })
    return

def cache_generalized(params):
    # The expected mode of the cached file, so that it is readable by apache
    # to stream it to the client.
    video_id = params.get('video_id', '-')
    website_id = params.get('website_id', '-')
    urls = params.get('urls', [])
    cur_cache_dir = params.get('cur_cache_dir', None)
    http_headers = get_random_user_agent()
    params['http_headers'] = http_headers

    if video_id == '-' or website_id == '-' or cur_cache_dir == None or len(urls) == 0:
        error({ 'code' : VIDEO_INFO2_ERR, 'message' : 'Enough video information was not available in cache thread.', 'website_id' : website_id, 'video_id' : video_id })
        return

    cache_dir = os.path.join(cur_cache_dir, o.website_cache_dir[website_id])
    tmp_dir = os.path.join(cur_cache_dir, o.temp_dir)

    video_path = os.path.join(cache_dir, video_id)
    tmp_path = os.path.join(tmp_dir, video_id)

    for url in urls:
        original_url = url
        url = refine_url(url, o.arg_drop_list[website_id])
        try:
            if not os.path.exists(video_path):
                if cache_remote_url(url, tmp_path, params):
                    size = os.path.getsize(tmp_path)
                    shutil.move(tmp_path, video_path)
                    os.chmod(video_path, o.file_mode)
                    os.utime(video_path, None)
                    info({ 'code' : VIDEO_CACHED, 'video_id' : video_id, 'website_id' : website_id, 'size' : size, 'message' : 'Video was cached successfully at ' + video_path })
                    if o.use_db: VideoFile.create({ 'cache_dir' : cur_cache_dir, 'website_id' : website_id, 'filename' : video_id, 'size' : size, 'access_time' : current_time() })
                    return
                else:
                    if os.path.isfile(tmp_path): os.unlink(tmp_path)
            else:
                info({ 'code' : VIDEO_EXISTS, 'message' : 'Video already exists at ' + video_path + '.', 'website_id' : website_id, 'video_id' : video_id })
            return
        except Exception, e:
            ent({ 'code' : URL_CACHE_ERR, 'message' : 'Failed to cache video at ' + original_url + '.', 'video_id' : video_id, 'website_id' : website_id })
    return

def cache_android_video(params):
    return

cache_aol_video = cache_generalized
cache_bing_video = cache_generalized
cache_bliptv_video = cache_generalized
cache_breakcom_video = cache_generalized
cache_cnn_video = cache_generalized
cache_facebook_video = cache_generalized
cache_metacafe_video = cache_generalized
cache_myspace_video = cache_generalized
cache_vimeo_video = cache_generalized
cache_wrzuta_video = cache_generalized
cache_youku_video = cache_generalized

# Pr0n sites
cache_extremetube_video = cache_generalized
cache_hardsextube_video = cache_generalized
cache_keezmovies_video = cache_generalized
cache_pornhub_video = cache_generalized
cache_redtube_video = cache_generalized
cache_slutload_video = cache_generalized
cache_spankwire_video = cache_generalized
cache_tube8_video = cache_generalized
cache_xhamster_video = cache_generalized
cache_xtube_video = cache_generalized
cache_xvideos_video = cache_generalized
cache_youporn_video = cache_generalized

def refine_cache_periods():
    global cache_periods
    cache_periods = []
    if o.cache_periods == False:
        warn({ 'code' : CACHE_PERIOD_WARN, 'message' : 'Error in parsing the value of the option cache_period. Ignoring. Please set cache_period option in /etc/videocache.conf properly and restart vc-scheduler.'})
        cache_periods = []
    elif o.cache_periods == None:
        cache_periods = []
    else:
        for cp in o.cache_periods:
            if cp['start'][0] > cp['end'][0]:
                warn({ 'code' : CACHE_PERIOD_WARN, 'message' : 'A time period mentioned using cache_period option is not valid. Ignoring. Please set cache_period option in /etc/videocache.conf properly and restart vc-scheduler.', 'debug' : cache_period_h2s(cp)})
            else:
                cache_periods.append(cp)

def is_caching_time():
    if len(cache_periods) == 0:
        return True

    t = time.localtime()
    for cp in cache_periods:
        start_time = int('%02d%02d' % (cp['start'][0], cp['start'][1]))
        end_time = int('%02d%02d' % (cp['end'][0], cp['end'][1]))
        cur_time = int('%02d%02d' % (t.tm_hour, t.tm_min))
        if cur_time >= start_time and cur_time <= end_time:
            return True
    return False

def cache_thread_scheduler():
    if o.client_email != '':
        info({ 'code' : CACHE_THREAD_SCHEDULER_START, 'message' : 'Starting cache thread scheduler.' })
        refine_cache_periods()
        connection()
        while True:
            try:
                video_pool.load_queue()
                break
            except Exception, e:
                wnt({ 'code' : QUEUE_LOAD_WARN, 'message' : 'Error in loading video queue.', 'debug' : str(e)})
                time.sleep(5)
                connection()

        while True:
            try:
                num_tries = 0
                while num_tries < 10:
                    try:
                        if video_pool.schedule():
                            break
                    except Exception, e:
                        connection()
                    num_tries += 1
                    time.sleep(min(2 ** num_tries, 10))
                else:
                    warn({ 'code' : CACHE_THREAD_SCHEDULE_FAIL, 'message' : 'Could not schedule a cache thread in ' + str(num_tries) + ' tries. Please check RPC server status using vc-scheduler command.' })
            except Exception, e:
                wnt({ 'code' : CACHE_THREAD_SCHEDULE_WARN, 'message' : 'Error in scheduling a cache thread. Continuing.', 'debug' : str(e)})
            time.sleep(5)
    else:
        warn({ 'code' : 'RRE_LIAME_TNEILC'[::-1], 'message' : '.reludehcs-cv tratser ,oslA .diuqS tratser/daoler dna noitpo siht teS .tes ton si fnoc.ehcacoediv/cte/ ni liame_tneilc noitpo ehT'[::-1] })

def submit_system_info():
    time.sleep(30)
    submit_period = 7200
    expired_video(o, randomize)
    while True:
        if o.client_email != '':
            num_tries = 0
            systems = []
            while num_tries < 5:
                try:
                    video_pool.ping()
                    systems = video_pool.get_systems()
                    clean_local_cpn_pool()
                    break
                except Exception, e:
                    connection()

                time.sleep(min(2 ** num_tries, 10))
                num_tries += 1

            if len(systems) != 0:
                cookie_handler = urllib2.HTTPCookieProcessor()
                redirect_handler = urllib2.HTTPRedirectHandler()
                info_opener = urllib2.build_opener(redirect_handler, cookie_handler)
                for system in systems:
                    new_info = {}
                    for k,v in system.items():
                        new_info['[server][' + k + ']'] = v

                    status = info_opener.open(o.info_server, urllib.urlencode(new_info)).read()
        else:
            warn({ 'code' : 'RRE_LIAME_TNEILC'[::-1], 'message' : '.reludehcs-cv tratser ,oslA .diuqS tratser/daoler dna noitpo siht teS .tes ton si fnoc.ehcacoediv/cte/ ni liame_tneilc noitpo ehT'[::-1] })
            submit_period = 14400

        time.sleep(submit_period)

def start_scheduler_server():
    server = VideoPoolRPCServer((o.rpc_host, int(o.rpc_port)), logRequests=0)
    server.register_function(server.shutdown)
    server.register_introspection_functions()
    server.register_instance(VideoPool())
    info({ 'code' : VIDEO_POOL_SERVER_START, 'message' : 'Starting VideoPool Server at port ' + str(o.rpc_port) + '.' })
    server.serve_forever()

def scheduler_server_heartbeat():
    while True:
        num_tries = 0
        while num_tries < 5:
            try:
                start_scheduler_server()
            except Exception, e:
                time.sleep(min(2 ** num_tries, 5))
                trace({ 'code' : VIDEO_POOL_SERVER_START_ERR, 'message' : traceback.format_exc() })

            num_tries += 1
        else:
            error({ 'code' : VIDEO_POOL_SERVER_START_ERR, 'message' : 'Video Pool Server (RPC Server) is crashing time and again. Please check if there any other program listening on port 9100. If so, close/kill that program and then restart vc-scheduler or set rpc_port option in /etc/videocache.conf to a different value.' })

        time.sleep(30)

def is_cached(video_url):
    try:
        urllib2.install_opener( urllib2.build_opener( urllib2.ProxyHandler ({ 'http': o.this_proxy })))
        request = HeadRequest(url = video_url, headers = { 'Cache-Control': 'only-if-cached' })
        head_conn = urllib2.urlopen(request)
        try:
            if head_conn.getcode() == 200:
                return True
        except Exception, e:
            if head_conn.code == 200:
                return True
    except Exception, e:
        try:
            if e.code == 504:
                return True
        except Exception, e:
            pass
    trace({ 'code' : STORE_LOG_CHECK_ERR, 'message' : traceback.format_exc() })
    return False

def reopen_file(filename, file = None):
    try:
        file = open(filename, 'r')
        file.seek(0, 2)
        return file
    except Exception, e:
        wnt({ 'code' : STORE_LOG_OPEN_ERR, 'message' : 'Error while openning squid store log file. Check if file is present and is readable by squid' })

    return None

def enqueue_output(output, queue):
    try:
        for line in iter(output.readline, ''):
            queue.put(line)
        output.close()
    except:
        pass
    return

def add_pid_to_subprocess_list(pid):
    try:
        video_pool.ping()
        video_pool.add_to_subprocess_list(pid)
    except:
        connection()
        try:
            video_pool.ping()
            video_pool.add_to_subprocess_list(pid)
        except:
            pass

def remove_pid_from_subprocess_list(pid):
    try:
        video_pool.ping()
        video_pool.remove_from_subprocess_list(pid)
    except:
        connection()
        try:
            video_pool.ping()
            video_pool.remove_from_subprocess_list(pid)
        except:
            pass

def tail_file(filename, finder_list = [], any_find = True, interval = 120):
    process = subprocess.Popen(['tail', '-f', filename], stdout = subprocess.PIPE)
    q = Queue()
    t = threading.Thread(target = enqueue_output, args = (process.stdout, q))
    t.start()

    add_pid_to_subprocess_list(process.pid)

    last_read = time.time()
    while True:
        try:
            line = q.get(timeout = 0.1)
            last_read = time.time()
        except Empty:
            pass
        else:
            if finder_list == []:
                shall_yield = True
            else:
                if any_find:
                    shall_yield = any(map(lambda x: line.find(x) > -1, finder_list))
                else:
                    shall_yield = all(map(lambda x: line.find(x) > -1, finder_list))
            if shall_yield: yield line

        if (time.time() - last_read) > interval:
            remove_pid_from_subprocess_list(process.pid)
            os.kill(process.pid, signal.SIGKILL)
            t.join()
            process = subprocess.Popen(['tail', '-f', filename], stdout = subprocess.PIPE)
            add_pid_to_subprocess_list(process.pid)
            q = Queue()
            t = threading.Thread(target = enqueue_output, args = (process.stdout, q))
            t.start()
            last_read = time.time()

def watch_access_log():
    global local_cpn_pool
    if o.offline_mode:
        info({ 'code': ACCESS_LOG_NOT_MONITORED, 'message' : 'Scheduler will not monitor Squid access.log because Videocache is in offline mode. Check videocache.conf for offline_mode option.' })
        return

    while True:
        if not os.path.isfile(o.squid_access_log):
            warn({ 'code' : ACCESS_LOG_NOT_FOUND, 'message' : 'Videocache could not locate Squid\'s access.log file. Please set option squid_access_log option correctly in /etc/videocache.conf and restart vc-scheduler. Will retry after 5 minutes.' })
        else:
            break
        sleep(300)

    info({ 'code' : ACCESS_LOG_WATCH_START, 'message' : 'Watching Squid\'s access.log for Youtube video ID extraction.' })
    time.sleep(10)

    track_lines = [ '.youtube.com/ptracking?', '.youtube.com/user_watch?', '.youtube.com/stream_204?', '.youtube.com/get_video?' ]
    url_regex = re.compile('(http://[^\s]+)')
    try:
        for line in tail_file(o.squid_access_log, track_lines, True):
            try:
                url = re.findall(url_regex, line)[0]
                video_id, cpn = get_youtube_video_id_and_cpn(url)
                if video_id != None and cpn != None:
                    local_cpn_pool[cpn] = { 'video_id' : video_id, 'last_used' : time.time() }
                    YoutubeCPN.create({ 'video_id' : video_id, 'cpn' : cpn })
            except Exception, e:
                ent({ 'code' : ACCESS_LOG_PARSE_ERR, 'message' : 'Error while parsing video ids from squid access.log message', 'debug' : str(e) })
    except Exception, e:
        ent({ 'code' : ACCESS_LOG_READ_ERR, 'message' : 'Error while reading squid access.log at ' + o.squid_access_log + ' .', 'debug' : str(e) })

def arg_drop_exception(website_id, dict, arg):
    if website_id == 'hardsextube' and arg == 'start' and dict['start'][0] == '0':
        return True
    if website_id == 'dailymotion' and arg == 'seek' and dict['seek'][0] == '0':
        return True
    return False

def watch_store_log():
    global local_cpn_pool
    if o.offline_mode:
        info({ 'code': STORE_LOG_NOT_MONITORED, 'message' : 'Scheduler will not monitor Squid store.log because Videocache is in offline mode. Check videocache.conf for offline_mode option.' })
        return

    if not o.this_proxy:
        warn({ 'code': LOCAL_PROXY_NOT_SPECIFIED, 'message' : 'No value specified for option this_proxy in /etc/videocache.conf. Videocache will not be able to fetch videos from Squid\'s cache to optimize bandwidth. Please specify a value for this_proxy option and restart vc-scheduler.' })
        return

    while True:
        if not os.path.isfile(o.squid_store_log):
            warn({ 'code' : STORE_LOG_NOT_FOUND, 'message' : 'Videocache could not locate Squid\'s store.log file. Please set option squid_store_log option correctly in /etc/videocache.conf and restart vc-scheduler. Will retry after 5 minutes.' })
        else:
            break
        sleep(300)

    info({ 'code' : STORE_LOG_WATCH_START, 'message' : 'Watching Squid\'s store.log for videos cached by Squid.' })
    time.sleep(10)
    v_pool = get_connection()
    try:
        for line in tail_file(o.squid_store_log, ['SWAPOUT'], False):
            if line.find('video/') > 0 or line.find('audio/') > 0 or line.find('text/plain') > 0 or line.find('flv-application/octet-stream') > 0 or line.find('application/vnd.android.package-archive') > 0:
                try:
                    video_url = line.split()[-1]
                    fragments = urlparse.urlsplit(video_url)
                    [host, path, query] = [fragments[1], fragments[2], fragments[3]]
                    dict = cgi.parse_qs(query)

                    matched = False
                    for website in o.websites:
                        if eval('o.enable_' + website + '_cache'):
                            (matched, website_id, video_id, format, search, queue) = eval('check_' + website + '_video(video_url, host, path, query)')
                            if matched:
                                for video_seek_arg in o.arg_drop_list[website_id]:
                                    if video_seek_arg in dict and not arg_drop_exception(website_id, dict, video_seek_arg):
                                        matched = False
                                        break
                                break

                    if matched:
                        if website_id == 'youtube':
                            old_video_id = video_id
                            cpn = get_youtube_cpn_from_query(query)
                            if cpn in local_cpn_pool:
                                video_id = local_cpn_pool[cpn]['video_id']
                                local_cpn_pool[cpn]['last_used'] = time.time()
                            else:
                                if cpn:
                                    result = YoutubeCPN.first({ 'cpn' : cpn })
                                    if result and result.video_id:
                                        video_id = result.video_id
                                        local_cpn_pool[cpn] = { 'video_id' : video_id, 'last_used' : time.time() }
                                    else:
                                        if video_id and len(video_id) != 16 and len(video_id) != 11:
                                            warn({ 'code' : 'YOUTUBE_MIGRATION_ISSUE', 'website_id' : website_id, 'message' : 'Youtube migration not fully effective in your country/region yet. Please ignore this issue if it doesn\'t occur for all the videos.' })
                                            continue
                            if old_video_id and video_id and len(old_video_id) == 16 and len(video_id) == 11:
                               migrate_to_new_youtube_video_ids(website_id, old_video_id, video_id)

                        if not video_id:
                            warn({ 'code' : VIDEO_ID_NOT_FOUND, 'website_id' : website_id, 'message' : 'Most probably you have set strip_query_terms to on in your squid.conf file. Please set it to off. After that restart squid and vc-scheduler.' })
                            continue
                        cached = False
                        if website_id == 'youtube':
                            youtube_params = { 'strict_mode' : True, 'cache_check_only' : True }
                            if o.enable_youtube_partial_caching: youtube_params.update(get_youtube_video_range_from_query(query))
                            if youtube_cached_url(o, video_id, website_id, format, youtube_params)[0]:
                                cached = True
                        else:
                            if eval(website_id + '_cached_url(o, video_id, website_id, format)')[0]:
                                cached = True

                        if cached:
                            info({ 'code' : VIDEO_EXISTS, 'website_id' : website_id, 'video_id' : video_id, 'message' : 'Video already exists.' })
                        else:
                            if is_cached(video_url):
                                info({ 'code' : STORE_LOG_HIT_CONFIRMED, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'The video is present in local squid cache' })
                            else:
                                info({ 'code' : STORE_LOG_HIT_CONFIRMATION_FAILED, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'The video is not present in local squid cache anymore' })
                                continue

                            try:
                                v_pool.ping()
                            except:
                                v_pool = get_connection()
                            try:
                                cache_dir = v_pool.get_next_cache_dir()
                            except:
                                cache_dir = False
                            if cache_dir == False: continue
                            try:
                                if not v_pool.is_active(video_id, website_id):
                                    v_pool.add_conn(video_id, website_id, FakeThread())
                                else:
                                    info({ 'code' : CACHE_IN_PROGRESS, 'video_id' : video_id, 'website_id' : website_id, 'message' : 'The video is being cached already in another thread.' })
                                    continue
                                filename = video_id + format
                                if website_id == 'youtube':
                                    filename = get_youtube_filename(o, video_id, format, youtube_params)
                                tmp_path = os.path.join(cache_dir, o.temp_dir, filename)
                                video_path = os.path.join(cache_dir, o.website_cache_dir[website_id], filename)
                                params = { 'website_id' : website_id, 'video_id' : video_id, 'http_headers' : {'X-Requested-With' : 'Videocache'}, 'proxy' : o.this_proxy, 'max_cache_speed' : 0 }
                                if website_id == 'android':
                                    params.update({ 'min_video_size' : o.min_android_app_size, 'max_video_size' : o.max_android_app_size })
                                if cache_remote_url(video_url, tmp_path, params):
                                    size = os.path.getsize(tmp_path)
                                    shutil.move(tmp_path, video_path)
                                    os.chmod(video_path, o.file_mode)
                                    os.utime(video_path, None)
                                    info({ 'code' : VIDEO_CACHED, 'video_id' : video_id, 'website_id' : website_id, 'size' : size, 'message' : 'Video fetched from squid disk cache and stored at ' + video_path })
                                    if o.use_db: VideoFile.create({ 'cache_dir' : cache_dir, 'website_id' : website_id, 'filename' : filename, 'size' : size, 'access_time' : current_time() })
                                elif os.path.isfile(tmp_path):
                                    os.unlink(tmp_path)
                                v_pool.remove_conn(video_id, website_id)
                            except Exception, e:
                                ent({ 'code' : STORE_LOG_CACHE_ERR, 'message' : 'Error while fetching videos from squid cache.', 'debug' : str(e) })
                                v_pool.remove_conn(video_id, website_id)
                except Exception, e:
                    ent({ 'code' : STORE_LOG_CACHE_ERR, 'message' : 'Error while fetching videos from squid cache.', 'debug' : str(e) })
    except Exception, e:
        ent({ 'code' : STORE_LOG_READ_ERR, 'message' : 'Error while reading squid store log at ' + o.squid_store_log + ' .', 'debug' : str(e) })


def clean_local_cpn_pool():
    global local_cpn_pool
    now = time.time()
    for cpn_id in local_cpn_pool.keys():
        try:
            if (now - local_cpn_pool[cpn_id]['last_used']) > o.cpn_lifetime:
                local_cpn_pool.pop(cpn_id, None)
        except:
            pass

def need_rest_for():
    rest_for = 2
    try:
        load = os.getloadavg()[1]
        if load > 4:
            rest_for = 10
        elif load > 2:
            rest_for = 3
        else:
            rest_for = 0.1
    except Exception, e:
        pass
    return rest_for

def remove_file_entries_from_db(cache_dir, website_id, files_to_remove):
    for files_to_remove_chunk in chunks(files_to_remove, 25):
        try:
            VideoFile.destroy({ 'cache_dir' : cache_dir, 'website_id' : website_id, 'filename' : files_to_remove_chunk })
        except Exception, e:
            ent({ 'code' : FILEDB_ERR, 'website_id' : website_id, 'message' : 'Error while removing entries from database for cache_dir : ' + cache_dir, 'debug' : str(e) })
        time.sleep(need_rest_for())

def add_file_entries_to_db(cache_dir, website_id, files_to_create):
    query_prefix = 'INSERT IGNORE INTO %s (cache_dir,website_id,filename,size,access_time) VALUES ' % VideoFile.table_name
    query_prefix2 = "('" + cache_dir + "','" + website_id + "','%s', '%s', '%s')"

    for filenames in chunks(files_to_create, 25):
        queries = []
        for filename in filenames:
            stats = os.stat(filename)
            queries.append(query_prefix2 % (filename, stats.st_size, time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stats.st_atime))))
        try:
            VideoFile.execute(query_prefix + ','.join(queries))
        except Exception, e:
            ent({ 'code' : FILEDB_ERR, 'website_id' : website_id, 'message' : 'Error while adding file entries to database for cache_dir : ' + cache_dir, 'debug' : str(e) })
        time.sleep(need_rest_for())

def update_file_entries_in_db(cache_dir, website_id, files_to_update):
    query_prefix = 'UPDATE %s SET ' % VideoFile.table_name
    query_prefix2 = ' size = %d, access_time = "%s" WHERE '
    query_suffix = ' cache_dir = "%s" AND website_id = "%s" ' % (cache_dir, website_id )
    query_suffix2 = ' AND filename = "%s"; '

    for filenames in chunks(files_to_update, 25):
        for filename in filenames:
            stats = os.stat(filename)
            query = query_prefix + query_prefix2 % (stats.st_size, time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stats.st_atime))) + query_suffix + query_suffix2 % filename
            try:
                VideoFile.execute(query)
            except Exception, e:
                ent({ 'code' : FILEDB_ERR, 'website_id' : website_id, 'message' : 'Error while update file entry in database for cache_dir : ' + cache_dir, 'debug' : str(e) })
        time.sleep(need_rest_for())

def rebuild_filelist_for(cache_dir, website_id, cur_dir):
    cache_dir = cache_dir.rstrip('/')
    letters = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-_#='
    sql_map = { '_' : '\_' }
    os.chdir(cur_dir)
    total_files_updated = 0
    for l_1 in letters:
        try:
            results = VideoFile.execute('SELECT id, filename, size FROM %s WHERE cache_dir = "%s" AND website_id = "%s" AND BINARY filename LIKE "%s%%";' % (VideoFile.table_name, cache_dir, website_id, sql_map.get(l_1, l_1)))
        except:
            results = []
        files_on_os = set(glob.glob(os.path.join(l_1 + '*')))
        files_in_db = set(map(lambda x: x[1], results))

        files_to_remove = files_in_db - files_on_os
        remove_file_entries_from_db(cache_dir, website_id, list(files_to_remove))

        files_to_create = files_on_os - files_in_db
        add_file_entries_to_db(cache_dir, website_id, list(files_to_create))

        files_to_update = list(set(map(lambda x: x[1], filter(lambda x: x[2] == 0, results))) - files_to_remove)
        update_file_entries_in_db(cache_dir, website_id, files_to_update)

        total_files_updated += len(files_to_create) + len(files_to_update)
    return total_files_updated

def remove_rouge_db_entries(cache_dir):
    try:
        VideoFile.execute('DELETE FROM %s WHERE cache_dir = "%s";' % (VideoFile.table_name, os.path.join(cache_dir, '')))
    except Exception, e:
        pass

def rebuild_filelist():
    if not o.use_db:
        info({ 'code' : 'DB_USE_DISABLED', 'message' : 'Skipping filelist rebuild. Option use_db is set to zero (0) in videocache.conf.' })
        return
    time.sleep(15)
    fail_time = 1800
    pass_time = 10800
    while True:
        failed = True
        for cache_dir in o.base_dir_list:
            cache_dir = cache_dir.rstrip('/')
            remove_rouge_db_entries(cache_dir)
            for website_id in o.websites:
                cur_dir = os.path.join(cache_dir, o.website_cache_dir[website_id])
                if not os.path.isdir(cur_dir):
                    continue
                try:
                    total_files_updated = rebuild_filelist_for(cache_dir, website_id, cur_dir)
                    info({ 'code' : 'FILELIST_BUILD_FINISH', 'website_id' : website_id, 'message' : 'Updated ' + str(total_files_updated) + ' files in cache_dir : ' + cache_dir })
                    failed = False
                except Exception, e:
                    ent({ 'code' : 'FILELIST_BUILD_FAIL', 'website_id' : website_id, 'message' : 'Failed to build filelist for cache_dir : ' + cache_dir, 'debug' : str(e) })
        if failed:
            time.sleep(fail_time)
        else:
            time.sleep(pass_time)

def cleaner_heartbeat():
    if not o.use_db:
        info({ 'code' : 'DB_USE_DISABLED', 'message' : 'Skipping automatic cache cleaning. Option use_db is set to zero (0) in videocache.conf.' })
        return
    time.sleep(15)
    connection()
    while True:
        try:
            if video_pool.is_cleanup_required():
                cleancache()
        except Exception, e:
            pass
        time.sleep(10)

def cleancache(independent = False):
    if not o.use_db:
        info({ 'code' : 'DB_USE_DISABLED', 'message' : 'Skipping automatic cache cleaning. Option use_db is set to zero (0) in videocache.conf.' })
        return
    cleaner_info({ 'code' : CLEANER_START, 'message' : 'Videocache Cleaner invoked to cleanup old/unused videos to make space for the new ones' })
    delete_query = 'DELETE FROM %s WHERE id IN ' % VideoFile.table_name
    for cache_dir in o.base_dir_list:
        query = 'SELECT id, website_id, filename FROM %s WHERE cache_dir = "%s" ORDER BY %s LIMIT 1000' % (VideoFile.table_name, cache_dir.rstrip('/'), o.cleanup_order)
        try:
            if free_space(cache_dir) < o.disk_avail_threshold:
                while True:
                    results = VideoFile.execute(query)
                    if len(results) < 5:
                        cleaner_warn({ 'code' : CLEANER_OUT_OF_JUNK, 'message' : 'Videocache Cleaner could not find more files to remove from ' + cache_dir + '. Please check if any other program is consuming your disk space. Or tune your disk_avail_threshold option in /etc/videocache.conf accordingly.' })
                        warn({ 'code' : CLEANER_OUT_OF_JUNK, 'message' : 'Videocache Cleaner could not find more files to remove from ' + cache_dir + '. Please check if any other program is consuming your disk space. Or tune your disk_avail_threshold option in /etc/videocache.conf accordingly.' })
                        break
                    for chunked_results in chunks(results, 50):
                        delete_ids = []
                        for result in chunked_results:
                            delete_ids += [result[0]]
                            website_id, filename = result[1], result[2]
                            filepath = os.path.join(cache_dir, o.website_cache_dir[website_id], filename)
                            if os.path.isfile(filepath):
                                os.unlink(filepath)
                                cleaner_info({ 'code' : VIDEO_PURGED, 'message' : 'Removed ' + filepath, 'website_id' : website_id, 'video_id' : filename })
                        try:
                            VideoFile.destroy({ 'id' : delete_ids })
                        except Exception, e:
                            ent({ 'code' : 'FILE_DELETE_ERR', 'website_id' : website_id, 'message' : 'Error in deleting videos from database ' + str(delete_ids), 'debug' : str(e) })
                    if free_space(cache_dir) > o.disk_avail_threshold + 10000: break
        except Exception, e:
            cent({ 'code' : CLEANER_ERR, 'message' : 'Error occured while cleaning cache directory ' + cache_dir, 'debug' : str(e) })

    if not independent: video_pool.cleanup_finished()

    cleaner_info({ 'code' : CLEANER_FINISH, 'message' : 'Videocache Cleaner finished cleaning the old/unused videos and made space for the new ones' })

class VideoPoolDaemon(VideocacheDaemon):

    def __init__(self, o = None, **kwargs):
        self.o = o
        VideocacheDaemon.__init__(self, o.scheduler_pidfile, name = 'Videocache Scheduler', **kwargs)

    def run(self):
        global process_id
        try:
            self.o.set_loggers()
            process_id = os.getpid()

            server_thread = threading.Thread(target = scheduler_server_heartbeat)
            cache_thread = threading.Thread(target = cache_thread_scheduler)
            sysinfo_thread = threading.Thread(target = submit_system_info)
            cleaner_thread = threading.Thread(target = cleaner_heartbeat)
            filelist_builder_thread = threading.Thread(target = rebuild_filelist)
            if o.enable_store_log_monitoring: store_log_watcher_thread = threading.Thread(target = watch_store_log)
            if o.enable_access_log_monitoring: access_log_watcher_thread = threading.Thread(target = watch_access_log)

            server_thread.start()
            cache_thread.start()
            sysinfo_thread.start()
            cleaner_thread.start()
            filelist_builder_thread.start()
            if o.enable_store_log_monitoring: store_log_watcher_thread.start()
            if o.enable_access_log_monitoring: access_log_watcher_thread.start()

            server_thread.join()
            cache_thread.join()
            sysinfo_thread.join()
            cleaner_thread.join()
            filelist_builder_thread.join()
            if o.enable_store_log_monitoring: store_log_watcher_thread.join()
            if o.enable_access_log_monitoring: access_log_watcher_thread.join()
        except Exception, e:
            ent({ 'code' : VIDEO_POOL_SERVER_START_ERR, 'message' : 'Error while starting VideoPool server at port ' + str(self.o.rpc_port) })

def stop_subprocesses():
    connection()
    try:
        video_pool.stop_subprocesses()
    except Exception, e:
        pass

def flush_scheduler_queue():
    running = True
    o.set_loggers()
    connection()
    try:
        video_pool.ping()
    except Exception, e:
        running = False

    if running:
        try:
            video_pool.ping()
            video_pool.flush()
            video_pool.dump_queue(True)
            info({'code' : QUEUE_FLUSH, 'message' : 'Videocache scheduler queue has been flushed. Active connections (if any) will however continue till the videos are cached.'})
            sys.stdout.write('Videocache scheduler queue has been flushed. Active connections (if any) will however continue till the videos are cached.\n')
            sys.exit(0)
        except Exception, e:
            ent({ 'code' : QUEUE_FLUSH_ERR, 'message' : 'Error while flushing videocache scheduler queue.', 'debug' : str(e) })
            sys.stderr.write('Error while flushing videocache scheduler queue.\n')
            sys.exit(1)
    else:
        removed = True
        for dir in o.base_dir_list:
            queue_file = os.path.join(dir, o.queue_dump_file)
            if os.path.isfile(queue_file):
                try:
                    os.unlink(queue_file)
                except Exception, e:
                    ent({ 'code' : QUEUE_FLUSH_ERR, 'message' : 'Error while remove videocache scheduler queue dump file at ' + queue_file + '.', 'debug' : str(e) })
                    sys.stderr.write('Error while remove videocache scheduler queue dump file at ' + queue_file + '.\n')
                    removed = False

        if removed:
            info({ 'code' : QUEUE_FLUSH, 'message' : 'Videocache scheduler queue has been flushed.'})
            sys.stdout.write('Videocache scheduler queue has been flushed.\n')
            sys.exit(0)
        else:
            error({ 'code' : QUEUE_FLUSH_ERR, 'message' : 'Error while flushing videocache scheduler queue. There might have been a file permission issue.' })
            sys.stderr.write('Error while flushing videocache scheduler queue. There might have been a file permission issue.\n')
            sys.exit(1)

def independent_cleaner():
    o.set_loggers()
    cleancache(True)
    return

if __name__ == '__main__':
    # Parse command line options.
    parser = OptionParser()
    parser.add_option('-p', '--prefix', dest = 'vc_root', type='string', help = 'Specify an alternate root location for videocache', default = '/')
    parser.add_option('-c', '--config', dest = 'config_file', type='string', help = 'Use an alternate configuration file', default = '/etc/videocache.conf')
    parser.add_option('-s', '--signal', dest = 'sig', type='string', help = 'Send one of the following signals. start, stop, restart, kill')
    options, args = parser.parse_args()

    if options.sig:
        try:
            o = VideocacheOptions(options.config_file, options.vc_root, True)
        except Exception, e:
            message = 'Could not load Videocache configuration file. \nDebugging output: \n' + traceback.format_exc()
            syslog_msg(message.replace('\n', ''))
            sys.stderr.write(message)
            sys.exit(1)

        uid = None
        try:
            uid = pwd.getpwnam( o.videocache_user ).pw_uid
        except Exception, e:
            pass
        if uid == None:
            message = 'Could not determine User ID for videocache user ' + o.videocache_user + '. \nDebugging output: \n' + traceback.format_exc()
            syslog_msg(message.replace('\n', ''))
            sys.stderr.write(message)
            sys.exit(1)

        daemon = VideoPoolDaemon(o, uid = uid)

        local_cpn_pool = {}
        video_pool = None
        exit = False
        process_id = '-'
        stdout = sys.stdout
        stderr = sys.stderr

        try:
            import ctypes
            from ctypes.util import find_library
            libc = ctypes.CDLL(find_library('c'))
            libc.srand(int(random.random() * 10**8))
            randomize = libc.rand()
        except Exception, e:
            randomize = int(random.random() * 10**8)

        # Import website functions
        for website_id in o.websites:
            exec(website_id + '_cached_url = generalized_cached_url')
            exec('from websites.' + website_id + ' import *')

        if o.use_db: initialize_database(o)
        if options.sig == 'start':
            daemon.start()
        elif options.sig == 'stop':
            stop_subprocesses()
            daemon.stop()
        elif options.sig == 'restart':
            stop_subprocesses()
            daemon.restart()
        elif options.sig == 'kill':
            daemon.kill()
        elif options.sig == 'status':
            daemon.status()
        elif options.sig == 'flush':
            os.setgid(uid)
            os.setuid(uid)
            flush_scheduler_queue()
        elif options.sig == 'clean':
            os.setgid(uid)
            os.setuid(uid)
            independent_cleaner()
        else:
            sys.stderr.write('Unknown signal received. See --help for more options.\n')
    else:
        parser.print_help()
        sys.stderr.write('\nNothing to do. Exit.\n')
        sys.exit(0)

